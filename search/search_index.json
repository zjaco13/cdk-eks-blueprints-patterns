{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EKS Blueprints Patterns","text":"<p>Welcome to the <code>EKS Blueprints Patterns</code> repository.</p> <p>This repository contains a number of samples for how you can leverage the Amazon EKS Blueprints. You can think of the patterns as \"codified\" reference architectures, which can be explained and executed as code in the customer environment.</p>"},{"location":"#patterns","title":"Patterns","text":"<p>The individual patterns can be found in the <code>lib</code> directory.  Most of the patterns are self-explanatory, for some more complex examples please use this guide and docs/patterns directory for more information.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Please refer to the Amazon EKS Blueprints Patterns documentation site for complete list of Amazon EKS Blueprints patterns documentation.</p> <p>Please refer to the Amazon EKS Blueprints Quick Start documentation site for complete project documentation.</p>"},{"location":"#usage","title":"Usage","text":"<p>Before proceeding, make sure AWS CLI is installed on your machine.</p> <p>To use the eks-blueprints and patterns module, you must have Node.js and npm installed. You will also use <code>make</code> and <code>brew</code> to simplify build and other common actions. </p>"},{"location":"#rhel-setup","title":"RHEL Setup","text":"<p>Follow the below steps to setup and leverage <code>eks-blueprints</code> and <code>eks-blueprints-patterns</code> in your Amazon Linux/CentOS/RHEL Linux machine.</p> <ol> <li> <p>Update the package list </p> <p>Update the package list to ensure you're installing the latest versions.</p> <pre><code>sudo yum update\n</code></pre> </li> <li> <p>Install <code>make</code></p> <p><pre><code>sudo yum install make\n</code></pre> 1. Install <code>brew</code> by following instructions as detailed in docs.brew.sh</p> </li> </ol> <pre><code> /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Add Homebrew to your PATH</p> <pre><code>test -d ~/.linuxbrew &amp;&amp; eval \"$(~/.linuxbrew/bin/brew shellenv)\"\ntest -d /home/linuxbrew/.linuxbrew &amp;&amp; eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\ntest -r ~/.bash_profile &amp;&amp; echo \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.bash_profile\necho \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.profile\n</code></pre> <p>Verify brew installation</p> <pre><code>brew -v\n</code></pre> <ol> <li> <p>Install <code>Node.js</code> and <code>npm</code></p> <p>Install Node.js v18 and npm using brew.</p> <pre><code>brew install node@18\n</code></pre> <p>Note: Node.js package includes npm</p> <p>Set PATH for node@18</p> <p><pre><code>test -r ~/.bash_profile &amp;&amp; echo 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.bash_profile\necho 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.profile\nexport PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"\n</code></pre> Post completing the above, continue from Verify Node.js and npm Installation</p> </li> </ol>"},{"location":"#ubuntu-setup","title":"Ubuntu Setup","text":"<p>Follow the below steps to setup and leverage <code>eks-blueprints</code> and <code>eks-blueprints-patterns</code> in your Ubuntu Linux machine.</p> <ol> <li> <p>Update the package list </p> <p>Update the package list to ensure you're installing the latest versions.</p> <pre><code>sudo apt update\n</code></pre> </li> <li> <p>Install <code>make</code></p> <pre><code>sudo apt install make\n</code></pre> </li> <li> <p>Install <code>brew</code> by following instructions as detailed in docs.brew.sh</p> </li> </ol> <pre><code> /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Add Homebrew to your PATH</p> <pre><code>test -d ~/.linuxbrew &amp;&amp; eval \"$(~/.linuxbrew/bin/brew shellenv)\"\ntest -d /home/linuxbrew/.linuxbrew &amp;&amp; eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\ntest -r ~/.bash_profile &amp;&amp; echo \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.bash_profile\necho \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.profile\n</code></pre> <p>Verify brew installation</p> <pre><code>brew -v\n</code></pre> <ol> <li> <p>Install <code>Node.js</code> and <code>npm</code></p> <p>Install Node.js v18 and npm using brew.</p> <pre><code>brew install node@18\n</code></pre> <p>Note: Node.js package includes npm</p> <p>Set PATH for node@18</p> <pre><code>test -r ~/.bash_profile &amp;&amp; echo 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.bash_profile\necho 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.profile\nexport PATH=\"/home/linuxbrew/.linuxbrew/opt/node@18/bin:$PATH\"\n</code></pre> </li> </ol> <p>Post completing the above, continue from Verify Node.js and npm Installation</p>"},{"location":"#mac-setup","title":"Mac Setup","text":"<p>Follow the below steps to setup and leverage <code>eks-blueprints</code> and <code>eks-blueprints-patterns</code> in your local Mac laptop.</p> <ol> <li>Install <code>make</code>, <code>node</code> and <code>npm</code> using brew</li> </ol> <p><pre><code>brew install make\nbrew install node@18\n</code></pre>     Note: Node.js package includes npm</p> <pre><code>Set PATH for node@18\n\n```bash\necho 'export PATH=\"/opt/homebrew/opt/node@18/bin:$PATH\"' &gt;&gt; ~/.zshrc\nexport PATH=\"/opt/homebrew/opt/node@18/bin:$PATH\"\n```\n</code></pre> <p>### Verify <code>Node.js</code> and <code>npm</code> Installation</p> <ol> <li> <p>Check the installed version of Node.js</p> <p><pre><code>node -v\n</code></pre> The output should be <code>v18.x.x</code>.</p> </li> <li> <p>Check the installed version of npm</p> <pre><code>npm -v\n</code></pre> <p>The output should be a version greater than <code>9.x.x</code>.</p> <p>If your npm version is not <code>9.x.x</code> or above, update npm with the following command:</p> <pre><code>sudo npm install -g npm@latest\n</code></pre> <p>Verify the installed version by running <code>npm -v</code>.</p> </li> </ol>"},{"location":"#repo-setup","title":"Repo setup","text":"<ol> <li> <p>Clone <code>cdk-eks-blueprints-patterns</code> repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>PS: If you are contributing to this repo, please make sure to fork the repo, add your changes and create a PR against it.</p> </li> <li> <p>Once you have cloned the repo, you can open it using your favourite IDE and run the below commands to install the dependencies and build the existing patterns.</p> </li> <li> <p>Install project dependencies.</p> <pre><code>make deps\n</code></pre> </li> <li> <p>To view patterns that are available to be deployed, execute the following:</p> <pre><code>npm i\nmake build\n</code></pre> </li> <li> <p>To list the existing CDK EKS Blueprints patterns</p> <pre><code>make list\n</code></pre> </li> </ol> <p>Note: Some patterns have a hard dependency on AWS Secrets (for example GitHub access tokens). Initially you will see errors complaining about lack of the required secrets. It is normal. At the bottom, it will show the list of patterns which can be deployed, in case the pattern you are looking for is not available, it is due to the hard dependency which can be fixed by following the docs specific to those patterns.</p> <pre><code>To work with patterns use: $ make pattern &lt;pattern-name&gt; &lt;list | deploy | synth | destroy&gt;\nExample:\n        $ make pattern fargate deploy \n\nPatterns: bottlerocket\n        data-at-rest\n        datadog\n        dynatrace-operator\n        ecr-image-scanning\n        emr\n        fargate\n        generic-cluster-provider\n        guardduty\n        jupyterhub\n        kasten\n        keptn-control-plane\n        kubecost\n        kubeflow\n        multi-region\n        multi-team\n        newrelic\n        nginx\n        pipeline-multienv-gitops\n        pipeline-multienv-monitoring\n        pipeline\n        rafay\n        secure-ingress-cognito\n        snyk\n        starter\n</code></pre> <ul> <li> <p>Bootstrap your CDK environment.</p> <pre><code>npx cdk bootstrap\n</code></pre> </li> <li> <p>You can then deploy a specific pattern with the following:</p> <pre><code>make pattern multi-team deploy\n</code></pre> </li> </ul>"},{"location":"#developer-flow","title":"Developer Flow","text":""},{"location":"#modifications","title":"Modifications","text":"<p>All files are compiled to the dist folder including <code>lib</code> and <code>bin</code> directories. For iterative development (e.g. if you make a change to any of the patterns) make sure to run compile:</p> <pre><code>make compile\n</code></pre> <p>The <code>compile</code> command is optimized to build only modified files and is fast. </p>"},{"location":"#new-patterns","title":"New Patterns","text":"<p>To create a new pattern, please follow these steps:</p> <ol> <li>Under lib create a folder for your pattern, such as <code>&lt;pattern-name&gt;-construct</code>. If you plan to create a set of patterns that represent a particular subdomain, e.g. <code>security</code> or <code>hardening</code>, please create an issue to discuss it first. If approved, you will be able to create a folder with your subdomain name and group your pattern constructs under it. </li> <li>Blueprints generally don't require a specific class, however we use a convention of wrapping each pattern in a plain class like <code>&lt;Pattern-Name&gt;Construct</code>. This class is generally placed in <code>index.ts</code> under your pattern folder. </li> <li>Once the pattern implementation is ready, you need to include it in the list of the patterns by creating a file <code>bin/&lt;pattern-name&gt;.ts</code>. The implementation of this file is very light, and it is done to allow patterns to run independently.</li> </ol> <p>Example simple synchronous pattern: <pre><code>import { configureApp } from '../lib/common/construct-utils';\nimport FargateConstruct from '../lib/fargate-construct';\n\nnew FargateConstruct(configureApp(), 'fargate'); // configureApp() will create app and configure loggers and perform other prep steps\n</code></pre></p> <ol> <li>In some cases, patterns need to use async APIs. For example, they may rely on external secrets that you want to validate ahead of the pattern deployment. </li> </ol> <p>Example async pattern:</p> <pre><code>import { configureApp, errorHandler } from '../lib/common/construct-utils';\n\nconst app = configureApp();\n\nnew NginxIngressConstruct().buildAsync(app, 'nginx').catch((e) =&gt; {\nerrorHandler(app, \"NGINX Ingress pattern is not setup. This maybe due to missing secrets for ArgoCD admin pwd.\", e);\n});\n</code></pre> <ol> <li>There are a few utility functions that can be used in the pattern implementation such as secret prevalidation. This function will fail if the corresponding secret is not defined, this preventing the pattern to deploy. </li> </ol> <pre><code>await prevalidateSecrets(NginxIngressConstruct.name, undefined, SECRET_ARGO_ADMIN_PWD); await prevalidateSecrets(\"my-pattern-name\", 'us-east-1', 'my-secret-name'); // \n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>See Contributing guide for requirements on contribution.</p>"},{"location":"#deploying-blueprints-with-external-dependency-on-aws-resources","title":"Deploying Blueprints with External Dependency on AWS Resources","text":"<p>There are cases when the blueprints defined in the patterns have dependencies on existing AWS Resources such as Secrets defined in the account/region. For such cases, you may see errors if such resources are not defined.</p> <p>For <code>PipelineMultiEnvGitops</code> please see instructions in this README.</p> <p>For <code>MultiRegionConstruct</code> the pattern relies on the following secrets defined:</p> <ol> <li><code>github-ssh-key</code> - must contain GitHub SSH private key as a JSON structure containing fields <code>sshPrivateKey</code> and <code>url</code>. The secret is expected to be defined in <code>us-east-1</code> and replicated to <code>us-east-2</code> and <code>us-west-2</code> regions. For more information on SSH credentials setup see ArgoCD Secrets Support. Example Structure:</li> </ol> <pre><code>{\n    \"sshPrivateKey\": \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\",\n\n    \"url\": \"git@github\"\n}\n</code></pre> <p>Note: You can notice explicit \\n characters in the sshPrivateKey.</p> <ol> <li><code>argo-admin-secret</code> - must contain ArgoCD admin password in Plain Text. The secret is expected to be defined in <code>us-east-1</code> and replicated to <code>us-east-1</code> and <code>us-west-2</code> regions.</li> </ol> <p>For <code>`Dynatrace One Agent</code></p> <ul> <li><code>dynatrace-tokens</code> - must contain API_URL, API_TOKEN and PAAS_TOKEN in Plain Text. The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication).</li> </ul> <p>For <code>keptn-control-plane</code> the pattern relies on the following secrets defined:</p> <ul> <li><code>keptn-secrets</code> - must contain API_TOKEN and BRIDGE_PASSWORD password in Plain Text. The secret is expected to be defined in <code>us-east-1</code> region.</li> </ul> <p>For <code>newrelic</code> the pattern relies on the following secrets defined:</p> <ul> <li><code>newrelic-pixie-keys</code> - must contain New Relic (required) and Pixie keys (optional). The secret is expected to be defined in the target region (either directly or through AWS Secrets Manager Replication).</li> </ul> <p>For more information on defining secrets for ArgoCD, please refer to Blueprints Documentation as well as known issues.</p> <p>For <code>nginx</code>  please see NGINX Blueprint documentation.</p> <p>For <code>datadog</code> the pattern relies on the following secret defined:</p> <ul> <li><code>apiKeyAWSSecret</code> - must contain the Datadog API key in Plain Text named <code>datadog-api-key</code>. The secret is expected to be defined in the target region.</li> </ul> <p>For <code>kubeflow</code> please see Kubeflow documentation.</p> <p>For <code>secure-ingress-cognito</code>  please see Secure Ingress using Cognito Blueprint documentation.</p>"},{"location":"#security","title":"Security","text":"<p>See CONTRIBUTING for more information.</p>"},{"location":"#license","title":"License","text":"<p>This library is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"patterns/backstage/","title":"Backstage on EKS","text":""},{"location":"patterns/backstage/#objective","title":"Objective","text":"<p>Backstage is an application that aims to facilitate introduction and maintenance of standards and best practices, across the organization, tying all infrastructure tooling, resources, owners, contributors, and administrators together in one place.</p> <p>The base functionality is provided by the Core component, which is assembled together with Plugins into an Application. Plugins extend the Core with additional functionalities that can be open source, or proprietary to a company.</p> <p>The objective of this pattern is to illustrate how to deploy a Backstage pre-built Docker image, using the Amazon EKS Blueprints Backstage add-on.</p>"},{"location":"patterns/backstage/#architecture","title":"Architecture","text":""},{"location":"patterns/backstage/#approach","title":"Approach","text":"<p>This blueprint will include the following:</p> <ul> <li>A new Well-Architected VPC with both Public and Private subnets</li> <li>A new Well-Architected EKS cluster in the region and account you specify</li> <li>An Application Load Balancer (ALB), implementing the Backstage Ingress rules</li> <li>An Amazon RDS for PostgreSQL instance</li> <li>A certificate, assigned to the ALB</li> <li>A Secret in AWS Secrets Manager, storing the database credentials, imported into the cluster via ExternalsSecretsAddOn</li> <li>Other popular add-ons</li> </ul>"},{"location":"patterns/backstage/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ul> <li>aws cli (also ensure it is configured)</li> <li>cdk</li> <li>npm</li> <li>tsc</li> <li>make</li> <li>Docker</li> </ul> <p>Let\u2019s start by setting the account and region environment variables:</p> <pre><code>ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nAWS_REGION=$(aws configure get region)\n</code></pre> <p>Create the Backstage application, command reported here for your convenience:</p> <pre><code>npx @backstage/create-app@latest\n</code></pre> <p>Build the corresponding Docker image, commands reported here for your convenience:</p> <pre><code>cd ./backstage\nyarn install --frozen-lockfile\nyarn tsc\nyarn build:backend --config app-config.yaml\n</code></pre> <p>Note: if the above command throws an error caused by app-config.yaml not found, you can explicitly set the path to the file:</p> <p><pre><code>yarn build:backend --config $(pwd)/app-config.yaml\n</code></pre> Then you can progress with the docker image build:</p> <pre><code>docker image build . -f packages/backend/Dockerfile --tag backstage\n</code></pre> <p>Note: consider the platform you are building on, and the target platform the image will run on, you might want to use the --platform option, e.g.:</p> <pre><code>docker buildx build ... --platform=...\n</code></pre> <p>Note: If you are running a version of Docker Engine version earlier than 23.0, you might need to enable BuildKit manually, like explained in the Getting Started section of the BuildKit webpage.</p> <p>(Optional) to show examples on the UI, add to Docker file:</p> <pre><code>COPY --chown=node:node examples /examples\n</code></pre> <p>Create an Amazon Elastic Container Registry (ECR) repository, named backstage:</p> <pre><code>aws ecr create-repository --repository-name backstage\n</code></pre> <pre><code>DOCKER_IMAGE_ID=... #see output of image id from above image creation\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\ndocker tag $DOCKER_IMAGE_ID $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/backstage:latest\ndocker push $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/backstage:latest\n</code></pre> <p>Setup a Hosted Zone in Route 53, with your parent domain. The pattern will create a new subdomain with format {backstage subdomain label}.{parent domain}. The default value for {backstage subdomain label} is backstage (see parameters below).</p>"},{"location":"patterns/backstage/#deployment","title":"Deployment","text":"<p>Clone the repository:</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Set the pattern's parameters in the CDK context by overriding the cdk.json file (edit PARENT_DOMAIN_NAME as it fits):</p> <pre><code>PARENT_DOMAIN_NAME=example.com\nHOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name $PARENT_DOMAIN_NAME --query \"HostedZones[].Id\" --output text | xargs basename)\ncat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"backstage.image.registry.name\": \"${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com\",\n        \"backstage.parent.domain.name\":\"${PARENT_DOMAIN_NAME}\",\n        \"backstage.hosted.zone.id\": \"${HOSTED_ZONE_ID}\"\n      }\n}\nEOF\n</code></pre> <p>(Optional) The full list of parameters you can set in the context is:</p> <pre><code>    \"context\": {\n        \"backstage.namespace.name\": ...,\n        \"backstage.image.registry.name\": ...,\n        \"backstage.image.repository.name\": ...,\n        \"backstage.image.tag.name\": ...,\n        \"backstage.parent.domain.name\": ...,\n        \"backstage.subdomain.label\": ...,\n        \"backstage.hosted.zone.id\": ...,\n        \"backstage.certificate.resource.name\": ...,\n        \"backstage.database.resource.name\": ...,\n        \"backstage.database.instance.port\": ...,\n        \"backstage.database.secret.resource.name\": ...,\n        \"backstage.database.username\": ...,\n        \"backstage.database.secret.target.name\": ...,\n      }\n</code></pre> <p>You can assign values to the above keys according to the following criteria (values are required where you don't see default mentioned):</p> <ul> <li>\"backstage.namespace.name\": Backstage's namespace, the default is \"backstage\"</li> <li>\"backstage.image.registry.name\": the image registry for the Backstage Helm chart in Amazon ECR, a value similar to \"youraccount.dkr.ecr.yourregion.amazonaws.com\"</li> <li>\"backstage.image.repository.name\": the image repository for the Backstage Helm chart, the default is \"backstage\"</li> <li>\"backstage.image.tag.name\": the image tag, the default is \"latest\"</li> <li>\"backstage.parent.domain.name\": the parent domain in your Hosted Zone</li> <li>\"backstage.subdomain.label\": to be used as {\"subdomain.label\"}.{\"parent.domain.name\"}, the default is \"backstage\"</li> <li>\"backstage.hosted.zone.id\": the Hosted zone ID (format: 20x chars/numbers)</li> <li>\"backstage.certificate.resource.name\": resource name of the certificate, registered by the resource provider, the default is \"backstage-certificate\"</li> <li>\"backstage.database.resource.name\": resource name of the database, registered by the resource provider, the default is \"backstage-database\"</li> <li>\"backstage.database.instance.port\": the port the database will use, the default is 5432</li> <li>\"backstage.database.secret.resource.name\": resource name of the database's Secret, registered by the resource provider, the default is \"backstage-database-credentials\"</li> <li>\"backstage.database.username\": the username for the database's credentials, the default is \"postgres\"</li> <li>\"backstage.database.secret.target.name\": the name to be used when creating the Secret, the default is \"backstage-database-secret\"</li> </ul> <p>If you haven't done it before, bootstrap your cdk account and region.</p> <p>Run the following commands:</p> <p><pre><code>make deps\nmake build\nmake pattern backstage deploy\n</code></pre> When deployment completes, the output will be similar to the following:</p> <p></p> <p>Navigate to the URL indicated by the first line in the output (_backstage-blueprint.BackstagebaseURL ...), you should see the screen below:</p> <p></p> <p>To see the deployed resources within the cluster, please run:</p> <pre><code>kubectl get pod,svc,secrets,ingress -A\n</code></pre> <p>A sample output is shown below:</p> <p></p>"},{"location":"patterns/backstage/#next-steps","title":"Next steps","text":"<p>You can go the AWS Blog to explore how to use Backstage e.g., as an API Developer Portal for Amazon API Gateway or to provision infrastructure using AWS Proton. On the Backstage website you can also see other examples of how to use and expand Backstage.</p>"},{"location":"patterns/backstage/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern backstage destroy </code></pre>"},{"location":"patterns/batch/","title":"AWS Batch on Amazon EKS Pattern","text":""},{"location":"patterns/batch/#objective","title":"Objective","text":"<p>AWS Batch helps you run batch computing workloads on AWS. Using Amazon EKS as the compute resource, you can now schedule and scale batch workloads into new or existing EKS cluster. As part of the deployment, AWS Batch doesn't create, administer, or perform lifecycle operations of the EKS cluster, but will only scale up and down the nodes maanged by AWS Batch and run pods on those nodes to complete batch jobs. </p> <p>The objective of this pattern is to deploy AWS Batch on Amazon EKS using EKS Blueprints with the following features in place: - Batch addon implemented - Batch Team defined with a sample compute environment and job queue (as defined under <code>lib/teams/team-batch</code>) - This can be customized based on your needs - Fluent Bit addon implemented to monitor AWS Batch on Amazon EKS jobs using CloudWatch, with the proper permissions for sending logs</p>"},{"location":"patterns/custom-networking-with-ipv4/","title":"Custom Networking on EKS","text":"<p>On Amazon EKS clusters, the default Container Networking Interface(CNI) is implemented by the Amazon VPC CNI plugin. When VPC CNI is used in EKS clusters, by default the VPC CNI assigns pods an IP address that's selected from the primary subnet of the VPC. The primary subnet is the subnet CIDR that the primary Elastic Network Interface(ENI) is attached to; usually it's the subnet of the worker node/host in the EKS cluster. If the primary subnet CIDR is too small, the CNI may not be able to have enough IP addresses to assign to the pods running in the cluster. This is a common challenge for EKS IPv4 clusters.</p> <p>Custom Networking provides a solution to the IP exhaustion issue by assigning the Pod IPs from secondary VPC address spaces(CIDR). When custom networking is enabled in VPC CNI, it creates secondary ENIs in the subnet defined under a custom resource named ENIConfig that includes an alternate subnet CIDR range (carved from a secondary VPC CIDR). The VPC CNI assigns pods IP addresses from the CIDR range defined in the ENIConfig Custom Resource Definition(CRD).</p> <p>Using the Custom Networking with IPv4 pattern, you should be able to stand up an EKS cluster with VPC CNI installed and configured with custom networking enabled.</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with a managed node group </li> <li>Deploys supporting add-ons: VpcCni, CoreDns, KubeProxy, AWSLoadBalancerController</li> <li>Enables Custom Networking configuration in VpcCni AddOn </li> </ul>"},{"location":"patterns/custom-networking-with-ipv4/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>yq</li> <li><code>make</code></li> </ol> <p>Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.</p>"},{"location":"patterns/custom-networking-with-ipv4/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":""},{"location":"patterns/custom-networking-with-ipv4/#check-versions","title":"Check Versions","text":"<p>Make sure that, following versions are installed. Node version is a current stable node version 18.x.</p> <p><pre><code>node -v\nv18.12.1\n</code></pre> NPM version must be 8.4 or above:</p> <pre><code>npm -v\n8.19.2\n</code></pre>"},{"location":"patterns/custom-networking-with-ipv4/#clone-the-cdk-blueprints-patterns-github-repository","title":"Clone the cdk-blueprints-patterns github repository","text":"<pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre>"},{"location":"patterns/custom-networking-with-ipv4/#install-project-dependencies","title":"Install project dependencies","text":"<p>Once you have cloned the above repository, you can open it using your favourite IDE and run the below command to install the dependencies and build the existing patterns.</p> <p><code>make deps</code></p>"},{"location":"patterns/custom-networking-with-ipv4/#to-view-patterns-that-are-available-to-be-deployed-execute-the-following","title":"To view patterns that are available to be deployed, execute the following:","text":"<pre><code>npm i\nmake build\n</code></pre> <p>To list the existing CDK EKS Blueprints patterns, run</p> <p><code>make list</code></p>"},{"location":"patterns/custom-networking-with-ipv4/#bootstrap-your-cdk-environment","title":"Bootstrap your CDK environment","text":"<p><code>npx cdk bootstrap</code></p> <p>You can now proceed with deployment of the <code>custom-networking-ipv4</code> pattern.</p>"},{"location":"patterns/custom-networking-with-ipv4/#to-deploy-the-custom-networking-ipv4-pattern-run","title":"To deploy the custom-networking-ipv4 pattern, run","text":"<p><code>make pattern custom-networking-ipv4 deploy</code></p> <p>Once the deployment is successful, run <code>update-kubeconfig</code> command to update the kubeconfig file with required access. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name custom-networking-ipv4-blueprint --region $AWS_REGION --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/custom-networking-ipv4-bl-customnetworkingipv4blue-2SR7PW3UBLIH\n</code></pre> <p>You can verify the resources created by executing</p> <pre><code>kubectl get node -o wide\n</code></pre> <p>Output:</p> <pre><code>NAME                                        STATUS   ROLES    AGE   VERSION                INTERNAL-IP   EXTERNAL-IP     OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-18-208.us-east-2.compute.internal   Ready    &lt;none&gt;   70m   v1.24.11-eks-a59e1f0   10.0.18.208   18.116.23.237   Amazon Linux 2   5.10.173-154.642.amzn2.x86_64   containerd://1.6.19\nip-10-0-61-228.us-east-2.compute.internal   Ready    &lt;none&gt;   70m   v1.24.11-eks-a59e1f0   10.0\n</code></pre>"},{"location":"patterns/custom-networking-with-ipv4/#under-the-hood","title":"Under the Hood","text":"<p>This pattern first creates secondary CIDRs and secondary subnets with specified range of CIDRs as shown below in resourceProvider. Then the VPC CNI addon sets up custom networking based on the parameters <code>awsVpcK8sCniCustomNetworkCfg</code>, <code>eniConfigLabelDef: \"topology.kubernetes.io/zone\"</code> for your Amazon EKS cluster workloads with secondary subnet ranges.</p> <ul> <li>When the secondary CIDRs are passed to the VPC resource provider, the secondary subnets are created and registered under names <code>secondary-cidr-subnet-${order}</code> with the resource providers.</li> <li>We enable CNI plugin with custom pod networking with below environment variables:<ul> <li><code>AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG</code> = <code>true</code></li> <li><code>ENI_CONFIG_LABEL_DEF</code> = <code>topology.kubernetes.io/zone</code></li> </ul> </li> </ul> <p>This deploys an ENIConfig custom resource for pod subnets (one per availability zone).</p> <p><pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nconst app = new cdk.App();\nconst addOn = new blueprints.addons.VpcCniAddOn({\n  customNetworkingConfig: {\n      subnets: [\n          blueprints.getNamedResource(\"secondary-cidr-subnet-0\"),\n          blueprints.getNamedResource(\"secondary-cidr-subnet-1\"),\n          blueprints.getNamedResource(\"secondary-cidr-subnet-2\"),\n      ]   \n  },\n  awsVpcK8sCniCustomNetworkCfg: true,\n  eniConfigLabelDef: 'topology.kubernetes.io/zone'\n});\nconst blueprint = blueprints.EksBlueprint.builder()\n  .addOns(addOn)\n  .resourceProvider(blueprints.GlobalResources.Vpc, new blueprints.VpcProvider(undefined, {\n                primaryCidr: \"10.2.0.0/16\", \n                secondaryCidr: \"100.64.0.0/16\",\n                secondarySubnetCidrs: [\"100.64.0.0/24\",\"100.64.1.0/24\",\"100.64.2.0/24\"]\n            }))\n  .build(app, 'my-stack-name');\n  ```\n\nIn the diagram shown below, a secondary CIDR (100/64) is assigned to each private subnet that gets created in an availability zone. Worker nodes in the EKS cluster still gets an IP address from the Primary CIDRs(10.0) range whereas the pods get an IP address from the secondary CIDR range.\n\n![Custom-NW-IPv4](./images/custom-nw-mng.png)\n\nThis can be verified by issuing the following command\n</code></pre> kubectl get eniconfig <pre><code>Output:\n</code></pre> NAME         AGE us-east-2a   47m us-east-2b   47m us-east-2c   47m <pre><code>An ENIConfig custom resource is created in each AZ.  Number of secondary ENIs associated with the Worker node varies by instance type.\n\n![Custom-NW-MNG](./images/custom-nw-mng.png)\n\n## Additional Configuration Options\n\nVPC CNI AddOn provides some knobs to add additional advanced configuration on top of custom networking.\n\n### Prefix Delegation\n\nWhen using custom networking mode, since the node\u2019s primary ENI is no longer used to assign Pod IP addresses, there is a decrease in the number of Pods that can run on a given EC2 instance type. To work around this limitation you can use prefix delegation with custom networking. This is an important capability because when you use custom networking, only Pods that are configured to use hostNetwork are \u201cbound\u201d to the host\u2019s primary ENI. All other Pods are bound to secondary ENIs. However, with prefix delegation enabled, each secondary IP is replaced with a /28 prefix which negates the IP address loss when you use custom networking.\n\nBy default, Prefix Delegation is turned off in Vpc Cni. To check this, run the following command.\n</code></pre> kubectl get ds aws-node -o yaml -n kube-system | yq '.spec.template.spec.containers[].env' <pre><code>Output:\n</code></pre> [...]</p> <ul> <li>name: ENABLE_PREFIX_DELEGATION</li> </ul> <p>value: \"false\"</p> <p>[...] <pre><code>Consider the maximum number of Pods for an m5.large instance with custom networking.\nWhen using custom networking, the maximum number of Pods you can run without prefix delegation enabled is 20.\n\nDownload and run max-pods-calculator.sh script to calculate the maximum number of pods:\n</code></pre> curl -o max-pods-calculator.sh https://raw.githubusercontent.com/awslabs/amazon-eks-ami/master/files/max-pods-calculator.sh chmod +x max-pods-calculator.sh /max-pods-calculator.sh \\     --instance-type m5.large \\     --cni-version 1.12.5-eksbuild.2 \\     --cni-custom-networking-enabled <pre><code>Output:\n</code></pre> 20 <pre><code>To turn on `Prefix Delegation`, use the following command\n</code></pre> kubectl set env daemonset aws-node -n kube-system ENABLE_PREFIX_DELEGATION=true <pre><code>Output:\n`110`\n\n![Custom-NW-Bar-Chart](./images/Custom-nw-bar-chart.png)\n\nThe reason we got max-pods is 110 instead of 290 is because the instance has a relatively low number of vCPUs. In addition the Kubernetes community recommends set max Pods no greater than 10 * number of cores, up to 110. Since Vpc Cni runs as a daemonset, you\u2019d need to create new nodes for this to take effect.\n\nThe number of ENIs and IP addresses in a pool are configured through environment variables called `WARM_ENI_TARGET`, `WARM_IP_TARGET`, `MINIMUM_IP_TARGET`. For more details on these options, please refer to [EKS Best Practices Networking](https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#overview) Guide.\n\n\n## Cleanup\n\nTo clean up your EKS Blueprints, run the following commands:\n\n\n```sh\nmake pattern custom-networking-ipv4 destroy \n</code></pre></p>"},{"location":"patterns/graviton/","title":"Graviton on EKS","text":"<p>AWS Graviton processors are designed by AWS to deliver the best price performance for your cloud workloads running in Amazon EC2. These processors are ARM chips running on aarch64 architecture.</p> <p>AWS Graviton processors are supported by many Linux operating systems including Amazon Linux 2, Red Hat Enterprise Linux, SUSE, and Ubuntu. Many popular applications and services for security, monitoring and management, containers, and continuous integration and delivery (CI/CD) from AWS and software partners also support AWS Graviton-based instances.</p> <p>AWS Graviton processors feature key capabilities that enable you to run cloud native applications securely, and at scale. EC2 instances powered by AWS Graviton processors are built on the AWS Nitro System that features the AWS Nitro security chip with dedicated hardware and software for security functions, and support for encrypted Amazon Elastic Block Store (EBS) volumes by default.</p>"},{"location":"patterns/graviton/#why-an-m7g-instance","title":"Why an M7g instance?","text":"<p>There are 7 families of Graviton instances split into 5 categories. General Purpose: M and T families Compute Optimized: C family Memory Optimized: R and X family Storage Optimized: I family Accelerated Computing: G family</p> <p>For a blueprint pattern, the General Purpose and Compute Optimized categories make the most sense, since they are the most common use cases and are all Nitro-Enabled instances.  Being Nitro-Enabled means that these instances provide better networking security as well as increased performance compared to non Nitro-Enabled instances.  In these categories, there are 7 different instance types: M7g, M6g, T4g, C7g, C7gn, C6g, and C6gn.  T4g instances are specialized for burstable workloads, and both T4g and M6g instances are Graviton2 chips. M7g instances are Graviton3 chips, which offer 25% better compute performace than Graviton2 and support DDR5 memory that provides 50% more bandwith compared to DDR4. C6g and C6gn instances are also Graviton2 chips, and C7g instances are specialized for high performance computing.  For this general blueprint pattern, the M7g instance is the best option due to the high compute power, memory bandwith, networking bandwith, and broad use cases.</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with a managed node group running on an M family Graviton3 processor</li> </ul>"},{"location":"patterns/graviton/#addons","title":"Addons","text":"<p>Not all of the listed EKS addons support the Graviton processors. To find a list of supported addons, visit the documentation.</p>"},{"location":"patterns/graviton/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li><code>make</code></li> </ol>"},{"location":"patterns/graviton/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy graviton pattern</p> <pre><code>make list\nnpx cdk bootstrap\nmake pattern graviton deploy\n</code></pre>"},{"location":"patterns/graviton/#verify-the-resources","title":"Verify the resources","text":"<p>Run the update-kubeconfig command. You should be able to get the command from the CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</p> <pre><code>aws eks update-kubeconfig --name graviton-blueprint --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/graviton-construct-bluepr-gravitonconstructbluepri-1OZNO42GH3OCB\n</code></pre> <p>Let's verify the resources created from the steps above.</p> <pre><code>kubectl get nodes -o json | jq -r '.items[] | \"Name: \",.metadata.name,\"\\nInstance Type: \",.metadata.labels.\"beta.kubernetes.io/instance-type\",\"\\nArch: \",.metadata.labels.\"beta.kubernetes.io/arch\",\"\\n\"' # Output shows node on M famGraviton3 processor and ARM architecture\n</code></pre>"},{"location":"patterns/graviton/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprint, run the following command:</p> <pre><code>make pattern graviton destroy\n</code></pre>"},{"location":"patterns/instana/","title":"IBM Instana on EKS pattern","text":"<p>The IBM\u00ae Instana\u00ae Addon for Amazon EKS Blueprint is designed to enhance observability, monitoring, and management capabilities for applications running on Amazon Elastic Kubernetes Service (EKS). Instana Addon focuses on enhancing the user experience by reducing the complexity and time required to install and configure an Instana host agent on Amazon EKS cluster.</p> <p>This Addon will use IBM\u00ae Instana\u00ae Agent Operator in the namespace <code>instana-agent</code> to install and manage Instana Agent. It also configures custom resource values to configure the operator.</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with a managed node group</li> <li>Install and set up Instana Agent for monitoring your EKS workloads. (by using the provided environment variable and additional configuration parameters)</li> </ul>"},{"location":"patterns/instana/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>Instana backend application - Use SaaS (eg aws) or Install self-hosted Instana backend (on-premises)</li> </ol>"},{"location":"patterns/instana/#project-setup","title":"Project Setup","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre> <p>Go inside project directory (eg. cdk-eks-blueprints-patterns)</p> <pre><code>cd cdk-eks-blueprints-patterns\n</code></pre> <p>Install project dependencies.</p> <pre><code>make deps\n</code></pre>"},{"location":"patterns/instana/#instana-agent-configuration","title":"Instana Agent Configuration","text":"<p>Go to your Instana Backend application (Instana User Interface), click ... More &gt; Agents &gt; Installing Instana Agents and select 'Kubernetes' platform to get the Instana Agent Key, Instana Service Endpoint, Instana Service port. These steps are also described here or in the screenshot below.</p> <p>Instana Agent Configuration</p>"},{"location":"patterns/instana/#usage-using-aws-secret-manager-secrets","title":"Usage : Using AWS Secret Manager Secrets","text":""},{"location":"patterns/instana/#aws-secret-manager-secrets-optional","title":"AWS Secret Manager Secrets (Optional)","text":"<p>If you wish to use AWS Secret Manager Secrets to pass Instana props (key, endpoint, and port), then you will be required to setup Secrets first.</p> <p><pre><code>export SECRET_NAME=&lt;aws_secret_name&gt;\nexport INSTANA_AGENT_KEY=&lt;instana_key&gt;\nexport INSTANA_ENDPOINT_HOST_URL=&lt;instana_host_endpoint&gt;\nexport INSTANA_ENDPOINT_HOST_PORT=&lt;instana_port&gt;\"\naws secretsmanager create-secret \\\n  --name $SECRET_NAME \\\n  --secret-string \"{\\\"INSTANA_AGENT_KEY\\\":\\\"${INSTANA_AGENT_KEY}\\\",\n    \\\"INSTANA_ENDPOINT_HOST_URL\\\":\\\"${INSTANA_ENDPOINT_HOST_URL}\\\",\n    \\\"INSTANA_ENDPOINT_HOST_PORT\\\":\\\"${INSTANA_ENDPOINT_HOST_PORT}\\\"\n}\"\n</code></pre> secret_name = AWS Secret Manager Secret name (eg. instana-secret-params).</p>"},{"location":"patterns/instana/#using-aws-secret-manager-secrets","title":"Using AWS Secret Manager Secrets","text":"<p>To use AWS Secret Manager Secrets follow these steps:</p> <ol> <li> <p>The actual settings for the secret name (<code>secretParamName</code>) are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory  <code>json \"context\": {      \"secretParamName\": \"instana-secret-params\"  }</code></p> </li> <li> <p>Go to project/lib/instana-construct/index.ts</p> </li> </ol> <pre><code>import { loadYaml } from \"@aws-quickstart/eks-blueprints/dist/utils\";\nimport * as cdk from \"aws-cdk-lib\";\nimport { InstanaOperatorAddon } from \"@instana/aws-eks-blueprint-addon\";\nimport { EksBlueprint, utils } from \"@aws-quickstart/eks-blueprints\";\nimport { prevalidateSecrets } from \"../common/construct-utils\";\n\nexport const instanaProps: { [key: string]: any } = {};\n\nexport default class InstanaConstruct {\nasync buildAsync(scope: cdk.App, id: string) {\ntry {\nawait prevalidateSecrets(InstanaConstruct.name, undefined, 'instana-secret-params');\n\nconst secretParamName: string = utils.valueFromContext(scope, \"secretParamName\", undefined);\nif(secretParamName != undefined) {\ninstanaProps.secretParamName = secretParamName;\n}\nconst yamlObject = loadYaml(JSON.stringify(instanaProps));\nconst stackId = `${id}-blueprint`;\nconst addOns = new InstanaOperatorAddon(yamlObject);\nEksBlueprint.builder()\n.account(process.env.CDK_DEFAULT_ACCOUNT!)\n.region(process.env.CDK_DEFAULT_REGION!)\n.addOns(addOns)\n.build(scope, stackId);\nconsole.log(\"Blueprint built successfully.\");\n} catch (error) {\nconsole.error(\"Error:\", error);\nthrow new Error(`environment variables must be setup for the instana-operator pattern to work`);\n}\n}\n}\n</code></pre>"},{"location":"patterns/instana/#usage-using-secrets-in-the-code","title":"Usage : Using Secrets in the Code","text":""},{"location":"patterns/instana/#setting-up-environment-variable","title":"Setting up environment variable","text":"<p>To set the following environment variables from the CLI, use the corresponding values obtained from the Instana Service Endpoint and Port (as shown in the above screenshot), and the Instana Application Key (also shown in the above screenshot):</p> <ul> <li>Set the value of INSTANA_ENDPOINT_HOST_URL to the Instana Service Endpoint.</li> <li>Set the value of INSTANA_ENDPOINT_HOST_PORT to the Instana Service Port.</li> <li>Set the value of INSTANA_AGENT_KEY to the Instana Application Key.</li> </ul> <p>Set the value of the following environment variable and run it on CLI to set those variables.</p> <p>For an example:</p> <pre><code>export INSTANA_AGENT_KEY=abc123\nexport INSTANA_ENDPOINT_HOST_URL=instana.example.com\nexport INSTANA_ENDPOINT_HOST_PORT=\"443\"\n</code></pre>"},{"location":"patterns/instana/#configure-additional-configuration-parameters","title":"Configure additional configuration parameters.","text":"<p>To configure additional parameters for Instana Agent according to your specific use case, follow these steps:</p> <ul> <li>Go to project/lib/instana-construct/index.ts</li> <li>Add the additional configuration parameters under <code>const instanaProps</code> variable.</li> </ul> <p>For an example:</p> <pre><code>export const instanaProps = {\nagent: {\nkey: process.env.INSTANA_AGENT_KEY,// Mandatory Parameter\nendpointHost: process.env.INSTANA_ENDPOINT_HOST_URL,//Mandatory Parameter\nendpointPort: process.env.INSTANA_ENDPOINT_HOST_PORT, // Mandatory Parameter,\nenv: {\nINSTANA_AGENT_TAGS: \"staging\",\n}\n}\n};\n</code></pre>"},{"location":"patterns/instana/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>To view patterns and deploy <code>instana-operator</code> pattern</p> <pre><code>make deps\nmake build\ncdk bootstrap\nmake pattern instana-operator deploy\n</code></pre>"},{"location":"patterns/instana/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/eks-blue1-eksblue1AccessRole32C5DF05-1NBFCH8INI08A\n</code></pre></p> <p>Lets verify the resources created by Steps above. <pre><code>kubectl get pods -n instana-agent # Output shows the EKS Managed Node group nodes under instana-agent namespace\n</code></pre> Output of the above command will be silimar to below one:</p> <p><pre><code>NAMESPACE       NAME                                                READY       STATUS    RESTARTS   AGE\ninstana-agent   controller-manager-78479cb596-sktg9     1/1         Running                     0          56m\ninstana-agent   controller-manager-78479cb596-xz8kn     1/1         Running                     0          56m\ninstana-agent   instana-agent-gsqx8                                 1/1         Running                     0          56m\n</code></pre> Run following command to verify Instana Agent logs <pre><code>kubectl logs &lt;instana-agent-pod-name&gt; -n instana-agent # Output shows instana agent logs. pod name in this example is instana-agent-gsqx8\n</code></pre></p>"},{"location":"patterns/instana/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>make pattern instana-operator destroy </code></pre>"},{"location":"patterns/instana/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open source NPM package aws-eks-blueprint-addon. Please refer to the package npm site for more information. <pre><code>https://www.npmjs.com/package/@instana/aws-eks-blueprint-addon'\n</code></pre> If you have any questions about the npm package or find any defect, please post in the source repo at: https://github.com/instana/instana-eks-blueprint-addon/issues</p>"},{"location":"patterns/jupyterhub/","title":"JupyterHub on EKS Pattern","text":""},{"location":"patterns/jupyterhub/#objective","title":"Objective","text":"<p>JupyterHub is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. The hub can offer notebook servers to a class of students, a corporate data science workgroup, a scientific research project, or a high-performance computing group.</p> <p>The objective of this pattern is to deploy JupyterHub on EKS using EKS Blueprints with the following features in place: - JupyterHub is hosted behind an ALB on EKS cluster across multiple AZs - JupyterHub allows for user friendly DNS name to route traffic to the load balancer, which is a subdomain of a parent domain in a separate account. This is representatitve of a typical global enterprise domain setup, where a central, global DNS account defines the parent domain (in Route53). The subdomain will be defined in Route53 from this account where the JupyterHub cluster is provisioned. - JupyterHub leverages an identity provider for user authentication. - JupyterHub uses persistent storage that is provided within a file system (i.e. EFS) when the user logs in - JupyterHub uses certificates to provide secured connection to the hub (the load balancer)  - The hub has a persistent storage with an EBS volume</p>"},{"location":"patterns/jupyterhub/#approach","title":"Approach","text":"<p>Since we will be defining subdomains for a global enterprise domain across multiple environments, which are as a rule placed in separate AWS accounts, root domain should defined in a separate account. Let's call it global DNS account. </p> <p>Our blueprint will then include the following:</p> <ol> <li>AWS Loadbalancer controller to provision an ALB instance fronting the Kubernetes Ingress resource for the JupyterHub server. Deployed with a public certificate created from ACM (Certificate ARN must be provided post-creation via CDK context)</li> <li>External DNS to integrate ALB with Route53 and use custom domain to access the hub. </li> <li>Configurations to leverage existing user management via OAuth 2.0 protocol standard (i.e. Auth0).</li> <li>EFS file server for user persistent storage using the Blueprints.</li> <li>EBS volume for hub persistent storage.</li> </ol>"},{"location":"patterns/jupyterhub/#prerequisites","title":"Prerequisites","text":"<ol> <li>Identity Provider that can be leveraged using 0Auth 2.0 protocol. The actual settings are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. Example settings: <pre><code>{\n  \"context\": {\n    \"callbackUrl\": \"https://your.hub.domain.com/hub/oauth_callback\",\n    \"authUrl\": \"https://some.auth.address.com/authorize\",\n    \"tokenUrl\": \"https://some.auth.address.com/oauth/token\",\n    \"userDataUrl\": \"https://some.auth.address.com/userinfo\",\n    \"clientId\": \"someClientID\",\n    \"clientSecret\": \"someClientSecret\",\n    \"scope\": [\"openid\",\"name\",\"profile\",\"email\"],\n    \"usernameKey\": \"name\"\n  }\n}\n</code></pre></li> <li>The parent domain must be defined in a separate account (GLOBAL_DNS_ACCOUNT).</li> <li>The GLOBAL_DNS_ACCOUNT must contain a role with a trust policy to the workload(s) account. We naed it <code>DomainOperatorRole</code> but you can choose any arbitrary name for it.</li> <li>Policies:  <code>arn:aws:iam::aws:policy/AmazonRoute53DomainsFullAccess</code> or alternatively you can provide <code>arn:aws:iam::aws:policy/AmazonRoute53ReadOnlyAccess</code> and <code>arn:aws:iam::aws:policy/AmazonRoute53AutoNamingFullAccess</code>.</li> <li>Trust relationship to allow workload accounts to create subdomains (replace <code>&lt;WORKLOAD_ACCOUNT&gt;</code> with the actual value):     <pre><code>{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n     {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n             \"AWS\": \"arn:aws:iam::&lt;WORKLOAD_ACCOUNT&gt;:root\"\n         },\n         \"Action\": \"sts:AssumeRole\",\n         \"Condition\": {}\n     }\n ]\n}\n</code></pre></li> <li>The actual settings for the GLOBAL_DNS_ACCOUNT, hosted zone name, subzone name, and the JupyterHub hub subdomain names are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. Example settings: <pre><code>{\n  \"context\": {\n    \"parent.dns.account\": \"&lt;PARENT_ACCOUNT&gt;\",\n    \"parent.hostedzone.name\": \"domain.com\",\n    \"dev.subzone.name\": \"hub.domain.com\",\n    \"jupyterhub.subzone.name\":\"your.hub.domain.com\",\n  }\n}\n</code></pre></li> </ol>"},{"location":"patterns/jupyterhub/#deploying","title":"Deploying","text":"<p>Once all pre-requisites are set you should be able to get a working cluster with all the objectives met, including a JupyterHub where users can log in using their credentials from the identity provider given.</p>"},{"location":"patterns/karpenter/","title":"Karpenter on EKS","text":"<p>Karpenter add-on is based on the Karpenter open source node provisioning project. It provides a more efficient and cost-effective way to manage workloads by launching just the right compute resources to handle a cluster's application.</p> <p>Karpenter works by:</p> <ul> <li>Watching for pods that the Kubernetes scheduler has marked as unschedulable,</li> <li>Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods,</li> <li>Provisioning nodes that meet the requirements of the pods,</li> <li>Scheduling the pods to run on the new nodes, and</li> <li>Removing the nodes when the nodes are no longer needed</li> </ul> <p>To learn more about Karpenter add on usage, please visit the documentation here This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with a managed node group</li> <li>Deploys supporting add-ons: AwsLoadBalancerController, VpcCni, CoreDns, KubeProxy,  CertManagerAddOn, KubeStateMetricsAddOn, MetricsServer</li> <li>Deploy Karpenter on the EKS cluster</li> </ul>"},{"location":"patterns/karpenter/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li><code>make</code></li> </ol>"},{"location":"patterns/karpenter/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy karpenter pattern</p> <pre><code>make list\nnpx cdk bootstrap\nmake pattern karpenter deploy\n</code></pre>"},{"location":"patterns/karpenter/#verify-the-resources","title":"Verify the resources","text":"<p>Run the update-kubeconfig command. You should be able to get the command from the CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</p> <pre><code>aws eks update-kubeconfig --name karpenter-blueprint --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/karpenter-construct-bluepr-karpenterconstructbluepri-1OZNO42GH3OCB\n</code></pre> <p>Let's verify the resources created from the steps above.</p> <pre><code># Assuming add-on is installed in the karpenter namespace.\n$ kubectl get po -n karpenter\nNAME                                          READY   STATUS    RESTARTS   AGE\nkarpenter-54fd978b89-hclmp   2/2     Running   0          99m\n</code></pre>"},{"location":"patterns/karpenter/#testing-with-a-sample-deployment","title":"Testing with a sample deployment","text":"<p>Now that the provisioner is deployed, Karpenter is active and ready to provision nodes. Create some pods using a deployment:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inflate\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: inflate\n  template:\n    metadata:\n      labels:\n        app: inflate\n    spec:\n      terminationGracePeriodSeconds: 0\n      containers:\n        - name: inflate\n          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2\n          resources:\n            requests:\n              cpu: 1\nEOF\n</code></pre> <p>Now scale the deployment:</p> <pre><code>kubectl scale deployment inflate --replicas 10\n</code></pre> <p>The provisioner will then start deploying more nodes to deploy the scaled replicas. You can verify by either looking at the karpenter controller logs,</p> <pre><code>kubectl logs -f -n karpenter karpenter-54fd978b89-hclmp\n</code></pre> <p>or, by looking at the nodes being created:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"patterns/kubeflow/","title":"Kubeflow on EKS","text":"<p>The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow.</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates EKS Cluster Control plane with public endpoint (for demo purpose only) with a managed node group</li> <li>Deploys supporting add-ons: ClusterAutoScaler, AwsLoadBalancerController, VpcCni, CoreDns, KubeProxy, EbsCsiDriver, CertManagerAddOn, KubeStateMetricsAddOn, PrometheusNodeExporterAddOn, AdotCollectorAddOn, AmpAddOn,</li> <li>Deploy Kubeflow on the EKS cluster</li> </ul>"},{"location":"patterns/kubeflow/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/kubeflow/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy kubeflow pattern</p> <pre><code>make list\ncdk bootstrap\nmake pattern kubeflow deploy\n</code></pre>"},{"location":"patterns/kubeflow/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/kubeflow-blueprint-kubeflowblueprintMastersRole0C1-saJBO\n</code></pre></p> <p>Let\u2019s verify the resources created by Steps above. <pre><code>kubectl get nodes # Output shows the EKS Managed Node group nodes\n\nkubectl get ns | kubeflow # Output shows kubeflow namespace\n\nkubectl get pods --namespace=kubeflow-pipelines  # Output shows kubeflow pods\n</code></pre></p>"},{"location":"patterns/kubeflow/#execute-machine-learning-jobs-on-kubeflow","title":"Execute Machine learning jobs on Kubeflow","text":"<p>log into Kubeflow pipeline UI by creating a port-forward to the ml-pipeline-ui service</p> <p><pre><code>kubectl port-forward svc/ml-pipeline-ui 9000:80 -n =kubeflow-pipelines\n</code></pre> and open this browser: http://localhost:9000/#/pipelines more pipeline examples can be found at https://www.kubeflow.org/docs/components/pipelines/tutorials/</p>"},{"location":"patterns/kubeflow/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>cdk destroy kubeflow-blueprint </code></pre>"},{"location":"patterns/kubeflow/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open source NPM package eks-blueprints-cdk-kubeflow-ext. Please refer to the package npm site for more information. https://www.npmjs.com/package/eks-blueprints-cdk-kubeflow-ext</p> <p>If you have any questions about the npm package or find any defect, please post in the source repo at  https://github.com/season1946/eks-blueprints-cdk-kubeflow-extension</p>"},{"location":"patterns/multi-account-monitoring/","title":"Multi Account Open Source Observability Pattern.","text":""},{"location":"patterns/multi-account-monitoring/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Multi Account Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana :</p> <p></p>"},{"location":"patterns/multi-account-monitoring/#objective","title":"Objective","text":"<ol> <li> <p>Deploying two production grade Amazon EKS cluster across 2 AWS Accounts ( Prod1, Prod2 account ) through a Continuous Deployment infrastructure pipeline triggered upon a commit to the repository that holds the pipeline configuration in an another AWS account (pipeline account).</p> </li> <li> <p>Deploying ADOT add-on, AMP add-on to Prod 1 Amazon EKS Cluster to remote write metrics to AMP workspace in Prod 1 AWS Account. Deploying ADOT add-on, CloudWatch add-on to Prod 1 Amazon EKS Cluster to write metrics to CloudWatch in Prod 2 AWS Account.</p> </li> <li> <p>Configuring GitOps tooling (ArgoCD addon) to support deployment of ho11y and yelb sample applications, in a way that restricts each application to be deployed only into the team namespace, by using ArgoCD projects.</p> </li> <li> <p>Setting up IAM roles in Prod 1 and Prod 2 Accounts to allow an AMG service role in the Monitoring account (4th AWS account) to access metrics from AMP workspace in Prod 1 account and CloudWatch namespace in Prod 2 account.</p> </li> <li> <p>Setting Amazon Managed Grafana to visualize AMP metrics from Amazon EKS cluster in Prod account 1 and CloudWatch metrics on workloads in Amazon EKS cluster in Prod account 2.</p> </li> </ol>"},{"location":"patterns/multi-account-monitoring/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p> <p>You can find the team-geordie configuration for this pattern in the workload repository under the folder <code>team-geordie</code>.</p>"},{"location":"patterns/multi-account-monitoring/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>AWS Control Tower deployed in your AWS environment in the management account. If you have not already installed AWS Control Tower, follow the Getting Started with AWS Control Tower documentation, or you can enable AWS Organizations in the AWS Management Console account and enable AWS SSO.</p> </li> <li> <p>An AWS account under AWS Control Tower called Prod 1 Account(Workloads Account A aka prodEnv1) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</p> </li> <li> <p>An AWS account under AWS Control Tower called Prod 2 Account(Workloads Account B aka prodEnv2) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</p> </li> <li> <p>An AWS account under AWS Control Tower called Pipeline Account (aka pipelineEnv) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</p> </li> <li> <p>An AWS account under AWS Control Tower called Monitoring Account (Grafana Account aka monitoringEnv) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</p> </li> </ol>"},{"location":"patterns/multi-account-monitoring/#deploying","title":"Deploying","text":"<ol> <li> <p>Fork this repository to your GitHub organisation/user.</p> </li> <li> <p>Clone your forked repository.</p> </li> <li> <p>Set environment variable <code>AWS_REGION</code> with region from where <code>pipelineEnv</code> account will be bootstrapped.</p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\n</code></pre> </li> <li> <p>Install the AWS CDK Toolkit globally on your machine using</p> <pre><code>npm install -g aws-cdk\n</code></pre> </li> <li> <p>Create secret <code>github-ssh-key</code> in <code>AWS_REGION</code> of <code>pipelineEnv</code> account. This secret must contain GitHub SSH private key as a JSON structure containing fields <code>sshPrivateKey</code> and <code>url</code> in <code>pipelineEnv</code> account. This will be used by ArgoCD addon to authenticate against any GitHub repository (private or public). The secret is expected to be defined in the region where the pipeline will be deployed to. For more information on SSH credentials setup see ArgoCD Secrets Support.</p> <pre><code>aws secretsmanager create-secret --region $AWS_REGION \\\n--name github-ssh-key \\\n--description \"SSH private key for ArgoCD authentication to GitHub repository\" \\\n--secret-string '{\n    \"sshPrivateKey\":\"&lt;SSH private key&gt;\",\n    \"url\":\"git@github\"\n}'\n</code></pre> </li> <li> <p>Create <code>github-token</code> secret in <code>AWS_REGION</code> of <code>pipelineEnv</code> account. This secret must be stored as a plain text in AWS Secrets Manager for the GitHub pipeline in <code>pipelineEnv</code> account. For more information on how to set it up, please refer to the docs. The GitHub Personal Access Token should have these scopes:</p> <ol> <li> <p>repo - to read the repository</p> </li> <li> <p>admin:repo_hook - if you plan to use webhooks (enabled by default)</p> </li> </ol> <pre><code>aws secretsmanager create-secret --region $AWS_REGION \\\n--name github-token \\\n--description \"GitHub Personal Access Token for CodePipeline to access GitHub account\" \\\n--secret-string \"&lt;GitHub Personal Access Token&gt;\"\n</code></pre> </li> <li> <p>Create secret <code>cdk-context</code> in <code>us-east-1</code> region as a plain text in AWS Secrets Manager for the GitHub pipeline in <code>pipelineEnv</code> account. <code>cdk-context</code> secret must be stored as a plain text in the following format in AWS Secrets Manager for cdk context for all the 4 AWS accounts used by the solution in <code>pipelineEnv</code> account. This secret must be created in <code>us-east-1</code> region.</p> <pre><code>aws secretsmanager create-secret --region us-east-1 \\\n--name cdk-context \\\n--description \"AWS account details of different environments used by Multi account open source Observability pattern\" \\\n--secret-string '{\n\"context\": {\n    \"prodEnv1\": {\n        \"account\": \"&lt;prodEnv1 account number&gt;\",\n        \"region\": \"&lt;AWS REGION&gt;\"\n    },\n    \"prodEnv2\": {\n        \"account\": \"&lt;prodEnv2 account number&gt;\",\n        \"region\": \"&lt;AWS REGION&gt;\"\n    },\n    \"pipelineEnv\": {\n        \"account\": \"&lt;pipelineEnv account number&gt;\",\n        \"region\": \"&lt;AWS REGION&gt;\"\n    },\n    \"monitoringEnv\": {\n        \"account\": \"&lt;prodmonitoringEnvEnv1 account number&gt;\",\n        \"region\": \"&lt;AWS REGION&gt;\"\n    }\n}\n}'\n</code></pre> </li> <li> <p>Create the following IAM users and attach <code>administrator</code> policy to required accounts.</p> <ol> <li> <p>IAM user <code>pipeline-admin</code> with <code>administrator</code> policy in Pipeline AWS Account</p> <pre><code>aws iam create-user \\\n[--profile pipelineEnv-admin-profile] \\\n--user-name pipeline-admin\n\naws iam attach-user-policy \\\n[--profile pipelineEnv-admin-profile] \\\n--user-name pipeline-admin \\\n--policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> </li> <li> <p>IAM user <code>prod1-admin</code> with <code>administrator</code> policy in Prod 1 AWS Account</p> <pre><code>aws iam create-user \\\n[--profile prodEnv1-admin-profile] \\\n--user-name prod1-admin\n\naws iam attach-user-policy \\\n[--profile prodEnv1-admin-profile] \\\n--user-name prod1-admin \\\n--policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> </li> <li> <p>IAM user <code>prod2-admin</code> with <code>administrator</code> policy in Prod 2 AWS Account</p> <pre><code>aws iam create-user \\\n[--profile prodEnv2-admin-profile] \\\n--user-name prod2-admin\n\naws iam attach-user-policy \\\n[--profile prodEnv2-admin-profile] \\\n--user-name prod2-admin \\\n--policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> </li> <li> <p>IAM user <code>mon-admin</code> with <code>administrator</code> policy in Monitoring AWS Account</p> <pre><code>aws iam create-user \\\n[--profile monitoringEnv-admin-profile] \\\n--user-name mon-admin\n\naws iam attach-user-policy \\\n[--profile monitoringEnv-admin-profile] \\\n--user-name mon-admin \\\n--policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n</code></pre> </li> <li> <p>IAM user <code>team-geordi</code> in Prod 1 and Prod 2 AWS Account</p> <pre><code>aws iam create-user \\\n[--profile prodEnv1-admin-profile] \\\n--user-name team-geordi\n\naws iam create-user \\\n[--profile prodEnv2-admin-profile] \\\n--user-name team-geordi        </code></pre> </li> <li> <p>IAM user <code>team-platform</code> in Prod 1 and Prod 2 AWS Account</p> <pre><code>aws iam create-user \\\n[--profile prodEnv1-admin-profile] \\\n--user-name team-platform\n\naws iam create-user \\\n[--profile prodEnv2-admin-profile] \\\n--user-name team-platform     </code></pre> </li> </ol> </li> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Bootstrap all 4 AWS accounts using step mentioned for different environment for deploying CDK applications in Deploying Pipelines. If you have bootstrapped earlier, please remove them before proceeding with this step. Remember to set <code>pipelineEnv</code> account number in <code>--trust</code> flag. You can also refer to commands mentioned below:</p> <pre><code># bootstrap prodEnv1 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n[--profile prodEnv1-admin-profile] \\\n--cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n--trust &lt;pipelineEnv account number&gt; \\\naws://&lt;prodEnv1 account number&gt;/$AWS_REGION\n\n# bootstrap prodEnv2 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n[--profile prodEnv2-admin-profile] \\\n--cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n--trust &lt;pipelineEnv account number&gt; \\\naws://&lt;prodEnv2 account number&gt;/$AWS_REGION\n\n# bootstrap pipelineEnv account WITHOUT explicit trust \nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n[--profile pipelineEnv-admin-profile] \\\n--cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\naws://&lt;pipelineEnv account number&gt;/$AWS_REGION\n\n# bootstrap monitoringEnv account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap \\\n[--profile monitoringEnv-admin-profile] \\\n--cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n--trust &lt;pipelineEnv account number&gt; \\\naws://&lt;monitoringEnv account number&gt;/$AWS_REGION\n</code></pre> </li> <li> <p>Modify the code of <code>lib/pipeline-multi-env-gitops/index.ts</code> and <code>lib/multi-account-monitoring/pipeline.ts</code> in your forked repo to point to your GitHub username/organisation. Look for the declared const of <code>gitOwner</code> and change it to your GitHub username and commit changes to your forked repo. This is needed because the AWS CodePipeline that will be automatically created will be triggered upon commits that are made in your forked repo.</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack in <code>pipelineEnv</code> account:</p> <pre><code>make build\nmake pattern pipeline-multienv-monitoring deploy multi-account-central-pipeline\n</code></pre> </li> <li> <p>Now you can go to AWS CodePipeline console, and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments. </p> </li> <li> <p>The deployment automation will create <code>ampPrometheusDataSourceRole</code> with permissions to retrieve metrics from AMP in Prod 1 Account, <code>cloudwatchDataSourceRole</code> with permissions to retrieve metrics from CloudWatch in Prod 2 Account and <code>amgWorkspaceIamRole</code> in monitoring account to assume roles in Prod 1 and Prod 2 account for retrieving and visualizing metrics in Grafana.</p> </li> <li> <p>Next, manually follow the following steps from AWS Open Source blog :</p> <ol> <li>AWS SSO in the management account</li> <li>Query metrics in Monitoring account from Amazon Managed Prometheus workspace in Prod 1 Account</li> <li>Query metrics in the Monitoring account from Amazon CloudWatch in Prod 1 Account</li> </ol> </li> </ol> <p></p> <p></p>"},{"location":"patterns/multi-account-monitoring/#validating-custom-metrics-and-traces-from-ho11y-app","title":"Validating Custom Metrics and Traces from ho11y App","text":"<ol> <li> <p>Run the below command in both clusters to generate traces to X-Ray and Amazon Managed Grafana Console out the sample <code>ho11y</code> app :</p> <pre><code>frontend_pod=`kubectl get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n        kubectl exec -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local;\n        echo ;\n        loop_counter=$[$loop_counter+1];\ndone\n</code></pre> </li> </ol> <p></p> <p></p>"},{"location":"patterns/multi-account-monitoring/#traces-and-service-map-screenshots-from-x-ray-console","title":"Traces and Service Map screenshots from X-Ray Console","text":""},{"location":"patterns/multi-account-monitoring/#custom-metrics-from-ho11y-app-on-amazon-managed-grafana-console-using-amp-as-data-source","title":"Custom Metrics from ho11y App on Amazon Managed Grafana Console using AMP as data source","text":""},{"location":"patterns/multi-account-monitoring/#custom-metrics-from-ho11y-app-on-amazon-managed-grafana-console-using-cloudwatch-as-data-source","title":"Custom Metrics from ho11y App on Amazon Managed Grafana Console using CloudWatch as data source","text":""},{"location":"patterns/multi-account-monitoring/#notes","title":"Notes","text":"<p>This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created by this pattern in Prod 1 and Prod 2 AWS Accounts. Make sure your account limits for EIP are increased to support additional 9 EIPs (1 per Subnets).</p>"},{"location":"patterns/nginx/","title":"NGINX Pattern","text":""},{"location":"patterns/nginx/#objective","title":"Objective","text":"<p>When setting up a target platform across multiple dimensions that question of ingress must be solved. Ideally, it should work in such as way that workloads provisioned on the target environments could be accessible via internet exposing sub-domains of some predefined global domain name. </p> <p>Communication with the workloads should leverage secure TLS protected Load balancer with proper public (or private) certificate.</p> <p>A single cluster will deploy workloads from multiple teams and each of them should be able to expose workloads routed to their corresponding namespace. So, teams are expected to define ingress objects. </p> <p>In addition, this approach should work not only for a single cluster, but also across multiple regions and environments. </p>"},{"location":"patterns/nginx/#approach","title":"Approach","text":"<p>Since we will be defining subdomains for a global enterprise domain across multiple environments, which are as a rule placed in separate AWS accounts, root domain should defined in a separate account. Let's call it global DNS account. </p> <p>Sub-domains are then defined in the target accounts (let's call them workload accounts).</p> <p>Our blueprint will then include the following:</p> <ol> <li>NGINX ingress controller to enable teams to create/configure their ingress objects. </li> <li>External DNS to integrate NGINX and public-facing NLB with Route53. </li> <li>AWS Loadbalancer controller to provision an NLB instance with each cluster fronting the NGINX ingress. Deployed with a public certificate that will also be provisioned as part of the blueprint.</li> <li>Team onboarding that leverage the ingress capabilities through ArgoCD. </li> <li>Other popular add-ons.</li> </ol>"},{"location":"patterns/nginx/#prerequisites","title":"Prerequisites","text":"<ol> <li><code>argo-admin-password</code> secret must be defined as plain text (not key/value) in <code>us-west-2</code>  region.</li> <li>The parent domain must be defined in a separate account (GLOBAL_DNS_ACCOUNT).</li> <li>The GLOBAL_DNS_ACCOUNT must contain a role with a trust policy to the workload(s) account. We naed it <code>DomainOperatorRole</code> but you can choose any arbitrary name for it.</li> <li>Policies:  <code>arn:aws:iam::aws:policy/AmazonRoute53DomainsFullAccess</code> or alternatively you can provide <code>arn:aws:iam::aws:policy/AmazonRoute53ReadOnlyAccess</code> and <code>arn:aws:iam::aws:policy/AmazonRoute53AutoNamingFullAccess</code>.</li> <li>Trust relationship to allow workload accounts to create subdomains (replace <code>&lt;WORKLOAD_ACCOUNT&gt;</code> with the actual value):     <pre><code>{\n \"Version\": \"2012-10-17\",\n \"Statement\": [\n     {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n             \"AWS\": \"arn:aws:iam::&lt;WORKLOAD_ACCOUNT&gt;:root\"\n         },\n         \"Action\": \"sts:AssumeRole\",\n         \"Condition\": {}\n     }\n ]\n}\n</code></pre></li> <li>The actual settings for the GLOBAL_DNS_ACCOUNT, hosted zone name and expected subzone name are expected to be specified in the CDK context. Generically it is inside the cdk.context.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. Example settings: <pre><code>{\n  \"context\": {\n    \"parent.dns.account\": \"&lt;PARENT_ACCOUNT&gt;\",\n    \"parent.hostedzone.name\": \"mycompany.a2z.com\",\n    \"dev.subzone.name\": \"dev.mycompany.a2z.com\",\n  }\n}\n</code></pre></li> </ol>"},{"location":"patterns/nginx/#deploying","title":"Deploying","text":"<p>Once all pre-requisites are set you should be able to get a working cluster with all the objectives met, including workloads with an example of team-specific ingress objects. </p>"},{"location":"patterns/paralus/","title":"Paralus on EKS","text":"<p>The Paralus project is a free open-source tool that enables controlled audited access to Kubernetes infrastructure. It comes with just-in-time service account creation and user-level credential management that integrates with your existing RBAC and SSO providers of choice. Learn more by visiting the offical documentation page: https://www.paralus.io/</p> <p>This pattern deploys the following resources:</p> <ul> <li>Creates a single EKS cluster with a public endpoint (for demo purpose only) that includes a managed node group</li> <li>Deploys supporting AddOn:  AwsLoadBalancerController, VpcCni, KubeProxy, EbsCsiDriverAddOn</li> <li>Deploy Paralus on the EKS cluster</li> </ul> <p>NOTE: Paralus installs a few dependent modules such as Postgres, Kratos, and also comes with a built-in dashboard. At it's core, Paralus works atop domain-based routing, inter-service communication, and supports the AddOns mentioned above.</p>"},{"location":"patterns/paralus/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/paralus/#deploy-an-eks-cluster-using-amazon-eks-blueprints-for-cdk","title":"Deploy an EKS Cluster using Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\n</code></pre> <p>Update FQDN information for your installation following the example below:</p> <pre><code>    fqdn: {\n\"domain\": \"yourdomain.com\",\n\"hostname\": \"console-eks\",\n\"coreConnectorSubdomain\": \"*.core-connector.eks\",\n\"userSubdomain\": \"*.user.eks\"\n}\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy the Paralus pattern, run the commands below:</p> <pre><code>cdk list\ncdk bootstrap\nmake pattern paralus deploy\n</code></pre>"},{"location":"patterns/paralus/#verify-the-resources","title":"Verify the resources","text":"<p>Run the update-kubeconfig command below. You should be able to get the command from the CDK output message once your cluster has been finished deploying. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</p> <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn arn:aws:iam::1234567890121:role/paralus-blueprint-paralusblueprintMastersRoleF3287-EI3XEBO1107B\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <pre><code>kubectl get nodes # Output will provide list of running nodes in your cluster\n\nkubectl get ns | grep paralus # Output shows Paralus namespace\n\nkubectl get pods --namespace=paralus-system  # Output shows Paralus pods\n\nblueprints-addon-paralus-contour-contour-7857f4cd9-kqhgp   1/1     Running                 \nblueprints-addon-paralus-contour-envoy-mx8z7               2/2     Running                 \nblueprints-addon-paralus-fluent-bit-525tt                  1/1     Running                 \nblueprints-addon-paralus-kratos-588775bc47-wf5gf           2/2     Running                 \nblueprints-addon-paralus-kratos-courier-0                  2/2     Running                 \nblueprints-addon-paralus-postgresql-0                      1/1     Running                 \ndashboard-6d8b54d78b-d8cks                                 1/1     Running                 \nparalus-66d9bbf698-qznzl                                   2/2     Running                 \nprompt-54d45cff79-h9x95                                    2/2     Running   \nrelay-server-79448564cb-nf5tj                              2/2     Running              </code></pre> <p>Learn more about the various components that are deployed as part of Paralus.</p>"},{"location":"patterns/paralus/#configure-dns-settings","title":"Configure DNS Settings","text":"<p>Once Paralus is installed, continue with following steps to configure DNS settings, reset default password and start using Paralus.</p> <p>Obtain the external ip address by executing below command against the installation <code>kubectl get svc blueprints-addon-paralus-contour-envoy -n paralus-system</code></p> <pre><code>NAME                            TYPE           CLUSTER-IP       EXTERNAL-IP                                                                     PORT(S)                         AGE\nblueprints-addon-paralus-contour-envoy         LoadBalancer   10.100.101.216   a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com   80:31810/TCP,443:30292/TCP      10m\n</code></pre> <p>Update the DNS settings to add CNAME records:</p> <pre><code>    name: console-eks value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n\n    name: *.core-connector.eks  value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n\n    name: *.user.eks value: a814da526d40d4661bf9f04d66ca53b5-65bfb655b5662d24.elb.us-west-2.amazonaws.com\n</code></pre> <p>Obtain your default password and reset it upon first login</p> <p><code>kubectl logs -f --namespace paralus-system $(kubectl get pods --namespace paralus-system -l app.kubernetes.io/name='paralus' -o jsonpath='{ .items[0].metadata.name }') initialize | grep 'Org Admin default password:'</code></p> <p>You can now access dashboard with http://console-eks.yourdomain.com ( refers to the hostname.domain specified during installation ), start importing clusters and using paralus.</p> <p>Note: you can also refer to this paralus eks blogpost</p>"},{"location":"patterns/paralus/#paralus-features-usage","title":"Paralus Features &amp; Usage","text":"<p>https://www.paralus.io/docs/usage/</p>"},{"location":"patterns/paralus/#configuring-centralized-kubectl-access-to-clusters","title":"Configuring centralized kubectl access to clusters","text":"<p>Kubectl is one of the most widely used tools to interact with Kubernetes. The command line tool allows you to deploy applications, inspect, and manage resources. It authenticates with the control plane for your cluster and makes API calls to the Kubernetes API. In short if you are working with Kubernetes - you will use kubectl the most.</p> <p>In most modern day scenarios, there are multiple users who are accessing various clusters. This makes it all more important to ensure that every user or group has access to only those resources that they are allowed to. A couple different approaches to achieve this include using namespaces and role based access control. While these are good, most enterprise grade application deployments require something more robust.</p> <p>That\u2019s where Paralus comes in. It allows you to configure centralized kubectl access to multiple clusters all from a single dashboard. It allows you to create groups, assign projects and users, and provide access. Check out this blog post for a deep dive into how you can use Paralus to import different clusters to Paralus and configure access to them using zero trust principles built in. Read More</p>"},{"location":"patterns/paralus/#cleanup","title":"Cleanup","text":"<p>To clean up your EKS Blueprints, run the following commands:</p> <pre><code>cdk destroy paralus-blueprint </code></pre>"},{"location":"patterns/paralus/#troubleshooting","title":"Troubleshooting","text":"<p>If postgres pvc is not getting a volume allocated, it probably is due to the iam permissions. Please refer this https://docs.aws.amazon.com/eks/latest/userguide/csi-iam-role.html to assign approriate policies to kubernetes sa</p>"},{"location":"patterns/paralus/#disclaimer","title":"Disclaimer","text":"<p>This pattern relies on an open-source NPM package paralus-eks-blueprints-addon. Please refer to the package npm site for more information. https://www.npmjs.com/package/@paralus/paralus-eks-blueprints-addon</p> <p>If you have any questions about the npm package or find any defect, please post in the source repo at  https://github.com/paralus/eks-blueprints-addon</p>"},{"location":"patterns/pipeline-multi-env-gitops/","title":"Pipeline Multi Environment Pattern","text":""},{"location":"patterns/pipeline-multi-env-gitops/#objective","title":"Objective","text":"<ol> <li>Deploying an EKS cluster across 3 environments( dev, test, and prod ), with a Continuous Deployment pipeline triggered upon a commit to the repository that holds the pipeline configuration.</li> <li>Configuring GitOps tooling (ArgoCD addon) to support multi-team and multi-repositories configuration, in a way that restricts each application to be deployed only into the team namespace, by using ArgoCD projects</li> </ol>"},{"location":"patterns/pipeline-multi-env-gitops/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository. The pattern uses the ECSDEMO applications as sample applications to demonstrate how to setup a GitOps configuration with multiple teams and multiple applications. The pattern include the following configurations in terms io:</p> <ol> <li>Application team - it defines 3 application teams that corresponds with the 3 sample applications used</li> <li>ArgoCD bootstrap - the pattern configure the ArgoCD addon to point to the workload repository of the EKS Blueprints samples</li> <li>ArgoCD projects - as part of the ArgoCD addon bootstrap, the pattern generate an ArgoCD project for each application team. The ArgoCD are used in order to restrict the deployment of an application to a specific target namespace</li> </ol> <p>You can find the App of Apps configuration for this pattern in the workload repository under the folder <code>multi-repo</code>.</p>"},{"location":"patterns/pipeline-multi-env-gitops/#prerequisites","title":"Prerequisites","text":"<ol> <li>Fork this repository to your GitHub organisation/user</li> <li>Clone your forked repository</li> <li> <p>Install the AWS CDK Toolkit globally on your machine using</p> <pre><code>npm install -g aws-cdk\n</code></pre> </li> <li> <p><code>github-ssh-key</code> - must contain GitHub SSH private key as a JSON structure containing fields <code>sshPrivateKey</code> and <code>url</code>. This will be used by ArgoCD addon to authenticate against ay GitHub repository (private or public). The secret is expected to be defined in the region where the pipeline will be deployed to. For more information on SSH credentials setup see ArgoCD Secrets Support.</p> </li> <li> <p><code>github-token</code> secret must be stored in AWS Secrets Manager for the GitHub pipeline. For more information on how to set it up, please refer to the docs. The GitHub Personal Access Token should have these scopes:</p> </li> <li>repo - to read the repository</li> <li> <p>admin:repo_hook - if you plan to use webhooks (enabled by default)</p> </li> <li> <p>Create the relevant users that will be used by the different teams</p> <pre><code>aws iam create-user --user-name frontend-user\naws iam create-user --user-name nodejs-user\naws iam create-user --user-name crystal-user\naws iam create-user --user-name platform-user\n</code></pre> </li> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>In case you haven't done this before, bootstrap your AWS Account for AWS CDK use using:</p> <pre><code>cdk bootstrap\n</code></pre> </li> <li> <p>Modify the code in your forked repo to point to your GitHub username/organisation. This is needed because the AWS CodePipeline that will be automatically created will be triggered upon commits that are made in your forked repo. Open the pattenrn file source code and look for the declared const of <code>gitOwner</code>. Change it to your GitHub username.</p> </li> <li> <p>OPTIONAL - As mentioned above, this pattern uses another repository for GitOps. This is the ArgoCD App of Apps configuration that resides in the aws-samples organisation. If you would like to modify the App of Apps configuration and customise it to your needs, then use the following instructions:</p> <ol> <li> <p>Fork the App of Apps workloads repo to your GitHub username</p> </li> <li> <p>Modify the pattern code with the following changes:</p> </li> <li> <p>Change the consts of <code>devArgoAddonConfig</code>, <code>testArgoAddonConfig</code>, and <code>prodArgoAddonConfig</code> to point to your GitHub username</p> </li> <li> <p>In the <code>createArgoAddonConfig</code> function, look for the <code>git@github.com:aws-samples/eks-blueprints-workloads.git</code> code under the <code>sourceRepos</code> configurations, and add another reference to your forked workload repository</p> </li> </ol> </li> </ol>"},{"location":"patterns/pipeline-multi-env-gitops/#deploying","title":"Deploying","text":"<p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> <pre><code>make pattern pipeline-multienv-gitops deploy eks-blueprint-pipeline-stack\n</code></pre> <p>Now you can go to AWS CodePipeline console, and see how it was automatically created to deploy multiple Amazon EKS clusters to different environments.</p>"},{"location":"patterns/pipeline-multi-env-gitops/#notes","title":"Notes","text":"<ol> <li> <p>In case your pipeline fails on the first run, it's because that the AWS CodeBuild step needs elevated permissions at build time. This is described in the official docs. To resolve this, locate <code>AccessDeniedException</code> in the CodeBuild build logs, and attach the following inline policy to it:</p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"VisualEditor0\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"sts:AssumeRole\",\n\"secretsmanager:GetSecretValue\",\n\"secretsmanager:DescribeSecret\",\n\"cloudformation:*\"\n],\n\"Resource\": \"*\"\n}\n]\n}\n</code></pre> </li> </ol> <p>The above inconvenience has been fixed in the Blueprints framework as well as in the pattern, so please report such cases if you encounter them. This item is left here for reference in case customers modify the pattern to require additional permissions at build time. </p> <ol> <li>This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created by this pattern. Make sure your account limit for EIP are increased to support additional 9 EIPs (1 per Subnets)</li> </ol>"},{"location":"patterns/secureingresscognito/","title":"Secure Ingress using Cognito Pattern","text":""},{"location":"patterns/secureingresscognito/#objective","title":"Objective","text":"<p>The objective of this pattern is to provide a secure authentication mechanism for customer applications using Amazon Cognito, ALB, and Route53, ensuring that only authorized users can access the application. The Kubecost tool is used as a reference or sample implementation to demonstrate the pattern's capabilities.</p> <p>To achieve this objective, the pattern utilizes Amazon Cognito to provide user authentication for the application's ingress, with ALB's built-in support for user authentication handling routine tasks such as user sign-up, sign-in, and sign-out. In addition to Amazon Cognito, ALB integrates with any OpenID Connect compliant identity provider (IdP) for a single sign-on experience across applications. ACM and Route53 provide SSL/TLS certificates to secure connections to ALB and authenticate users, preventing sensitive information from being intercepted or tampered with during transmission.</p> <p>The pattern also leverages Kubecost to provide real-time cost visibility and analysis for Kubernetes clusters, enabling customers to make informed decisions about resource allocation and utilization. This pattern can be easily adapted and extended to secure ingress for any application, providing a unified and secure solution for user authentication while optimizing costs. By implementing this solution, Amazon EKS customers can have a reliable, scalable, and secure authentication mechanism for their applications, with a cost optimization tool to manage and reduce the costs associated with their Kubernetes clusters.</p>"},{"location":"patterns/secureingresscognito/#architecture","title":"Architecture","text":""},{"location":"patterns/secureingresscognito/#approach","title":"Approach","text":"<p>This blueprint will include the following:</p> <ul> <li>A new Well-Architected VPC with both Public and Private subnets.</li> <li>A new Well-Architected EKS cluster in the region and account you specify.</li> <li>EBS CSI Driver Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes.</li> <li>AWS and Kubernetes resources needed to support AWS Load Balancer Controller.</li> <li>Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS.</li> <li>External-DNS allows integration of exposed Kubernetes services and Ingresses with DNS providers</li> <li>Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts</li> <li>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and bootstraping your workloads from public and private Git repositories.</li> <li>Create the necessary Cognito resources like user pool, user pool client, domain, Pre sign-up Lambda trigger and Pre authentication Lambda triggers  etc.., and passed to the Argo CD app of apps pattern from which ingress resources can reference.</li> </ul>"},{"location":"patterns/secureingresscognito/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstrap the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p>"},{"location":"patterns/secureingresscognito/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/secureingresscognito/#deploy","title":"Deploy","text":"<ol> <li>Let\u2019s start by setting a few environment variables. Change the Region as needed.</li> </ol> <pre><code>ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nAWS_REGION=us-west-2\n</code></pre> <ol> <li>Clone the repository and install dependency packages. This repository contains CDK v2 code written in TypeScript.</li> </ol> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\nnpm i\n</code></pre> <ol> <li>argo-admin-password secret must be defined as plain text (not key/value) in <code>us-west-2</code>  region.</li> </ol> <p><pre><code>aws secretsmanager create-secret --name argo-admin-secret \\\n    --description \"Admin Password for ArgoCD\" \\\n    --secret-string \"password123$\" \\\n    --region \"us-west-2\"\n</code></pre> 4. The CDK code expects the allowed domain and subdomain names in the CDK context file (cdk.json).</p> <p>Create two environment variables. The PARENT_HOSTED_ZONE variable contains your company\u2019s domain name. The DEV_SUBZONE_NAME will be the address for your Kubecost dashboard. </p> <p>Generate the cdk.json file:</p> <pre><code>PARENT_HOSTED_ZONE=mycompany.a2z.com\nDEV_SUBZONE_NAME=dev.mycompany.a2z.com\ncat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"parent.hostedzone.name\": \"${PARENT_HOSTED_ZONE}\",\n        \"dev.subzone.name\": \"${DEV_SUBZONE_NAME}\"\n      }\n}\nEOF\n</code></pre> <ol> <li>In this solution, we\u2019ll allow access to the Kubecost dashboard based on user email addresses. You can control access to the dashboard by allow-listing an entire domain or individual email addresses. </li> </ol> <p>Users are required to sign-up before they can access the Kubecost dashboard. The pre sign-up Lambda trigger only allows sign-ups when user\u2019s email domain matches allow-listed domains. When users sign-up, Cognito sends a verification code to their email address. Users have to verify access (using the one time valid code) to their email before they get access to the dashboard. </p> <p>If you\u2019d like to limit access to the dashboard by email addresses, you can also create a parameter to store allowed email addresses and add a logic to the pre authentication Lambda trigger.</p> <p>Create below parameters with allowed email addresses and domains in the AWS Systems Manager Parameter Store:</p> <pre><code>export SSM_PARAMETER_KEY=\"/secure-ingress-auth-cognito/ALLOWED_DOMAINS\"\nexport SSM_PARAMETER_VALUE=\"emaildomain1.com,emaildomain2.com\"\n\naws ssm put-parameter \\\n  --name \"$SSM_PARAMETER_KEY\" \\\n  --value \"$SSM_PARAMETER_VALUE\" \\\n  --type \"String\" \\\n  --region $AWS_REGION\n</code></pre> <ol> <li>Execute the commands below to bootstrap the AWS environment in <code>us-west-2</code></li> </ol> <pre><code>cdk bootstrap aws://$ACCOUNT_ID/$AWS_REGION\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern secure-ingress-cognito deploy secure-ingress-blueprint\n</code></pre>"},{"location":"patterns/secureingresscognito/#cluster-access","title":"Cluster Access","text":"<p>Once the deploy completes, you will see output in your terminal window similar to the following:</p> <p><pre><code>Outputs:\nsecure-ingress-blueprint.secureingressblueprintClusterNameD6A1BE5C = secure-ingress-blueprint\nsecure-ingress-blueprint.secureingressblueprintConfigCommandD0275968 =  aws eks update-kubeconfig \u2014name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::&lt;ACCOUNT ID&gt;:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0\nsecure-ingress-blueprint.secureingressblueprintGetTokenCommand21BE2184 =  aws eks get-token \u2014cluster-name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::&lt;ACCOUNT ID&gt;:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0\n</code></pre> <pre><code>Stack ARN:\narn:aws:cloudformation:us-west-2:&lt;ACCOUNT ID&gt;:stack/secure-ingress-blueprint/64017120-91ce-11ed-93b2-0a67951f5d5d\n</code></pre></p> <p>To update your Kubernetes config for your new cluster, copy and run the secure-ingress-blueprint.secureingressblueprintConfigCommandD0275968 command (the second command) in your terminal.</p> <pre><code>aws eks update-kubeconfig \u2014name secure-ingress-blueprint \u2014region us-west-2 \u2014role-arn arn:aws:iam::&lt;ACCOUNT ID&gt;:role/secure-ingress-blueprint-secureingressblueprintMas-7JD5S67SG7M0\n</code></pre> <p>Validate that you now have kubectl access to your cluster via the following:</p> <pre><code>kubectl get all -n kubecost\n</code></pre> <p>You should see output that lists all namespaces in your cluster.</p>"},{"location":"patterns/secureingresscognito/#test-authentication","title":"Test authentication","text":"<p>Point your browser to the URL of the Kubecost app in your cluster. You can get the URL from the cdk.json file using the below command.</p> <pre><code>awk -F':' '/dev.subzone.name/ {print $2}' cdk.json | tr -d '\",' | xargs echo\n</code></pre> <p>Your browser will be redirected to a sign-in page. This page is provided by Amazon Cognito hosted UI.</p> <p>Since this is your first time accessing the application, sign up as a new user. The data you input here will be saved in the Amazon Cognito user pool you created earlier in the post. </p> <p></p> <p>Select \u201cSign up\u201d and use your email address and create a password</p> <p></p> <p></p> <p>Use the verification code received in your email and confirm the account. Once you sign in, ALB will send you to the Kubecost app\u2019s UI:</p> <p></p> <p>Select the \u201cAWS Cluster #1\u201d to view the cost overview, savings and efficiency details.</p> <p></p>"},{"location":"patterns/windows/","title":"Windows Nodes on EKS","text":"<p>We (AWS) have received many requests to add windows node group support from the customers who run their workloads on Windows. Customers want to scale these workloads on Kubernetes alongside their Linux workloads. Amazon EKS supports windows node groups and you can Windows worker node group to an Amazon EKS cluster. This pattern Creates EKS Cluster Control plane with a managed node group running windows node. Please check our AWS doc on Enabling Windows support for your Amazon EKS cluster to learn more about considerations, prerequisites on running windows nodes with EKS cluster. Also please refer to this AWS doc to learn about Amazon EKS optimized Windows AMIs.</p>"},{"location":"patterns/windows/#addons","title":"Addons","text":"<p>Not all of the listed EKS addons support windows. We are currently working on a list of supported addons documentation which will be published here.</p>"},{"location":"patterns/windows/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li><code>make</code></li> </ol>"},{"location":"patterns/windows/#configuration-options","title":"Configuration Options","text":"<p>The pattern exposes the <code>WindowsBuilder</code> construct to build cluster with windows node groups. At the moment, adding windows nodes to the cluster requires at least one linux node group present to deploy core add-ons, such as VPC-CNI and CoreDNS. </p> <p>The <code>WindowsBuilder</code> provides a set of options, most of which are similar to managed node groups. </p> <p>In addition, it provides an attribute <code>noScheduleForWindowsNodes : true | false</code>. When set to <code>true</code> it will automatically add a <code>NoSchedule</code> taint to the Windows nodes. This approach is a safe way to disallow any application that does not provide proper tolerations to be scheduled on Windows nodes. </p> <p>In this scenario, in order to schedule a workload (application/add-on) on Windows nodes, customers can apply the following node selectors and tolerations to their deployments:</p> <pre><code>nodeSelector:\nkubernetes.io/os: windows\ntolerations:\n- key: \"os\"\noperator: \"Equal\"\nvalue: \"windows\"\neffect: \"NoSchedule\"\n</code></pre>"},{"location":"patterns/windows/#deploy-eks-cluster-with-amazon-eks-blueprints-for-cdk","title":"Deploy EKS Cluster with Amazon EKS Blueprints for CDK","text":"<p>Clone the repository</p> <pre><code>git clone https://github.com/aws-samples/cdk-eks-blueprints-patterns.git\ncd cdk-eks-blueprints-patterns\n</code></pre> <p>Updating npm</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To view patterns and deploy kubeflow pattern</p> <pre><code>make list\nnpx cdk bootstrap\nmake pattern windows deploy\n</code></pre>"},{"location":"patterns/windows/#verify-the-resources","title":"Verify the resources","text":"<p>Run the update-kubeconfig command. You should be able to get the command from the CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access</p> <pre><code>aws eks update-kubeconfig --name windows-eks-blueprint --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/windows-construct-bluepr-windowsconstructbluepri-1OZNO42GH3OCB\n</code></pre> <p>Let's verify the resources created from the steps above.</p> <pre><code>kubectl get nodes -o json | jq -r '.items[] | \"Name: \",.metadata.name,\"\\nInstance Type: \",.metadata.labels.\"beta.kubernetes.io/instance-type\",\"\\nOS Type: \",.metadata.labels.\"beta.kubernetes.io/os\",\"\\n\"' # Output shows Windows and Linux Nodes\n</code></pre>"},{"location":"patterns/windows/#deploy-sample-windows-application","title":"Deploy sample windows application","text":"<p>Create a namespace for the windows app called windows</p> <pre><code>kubectl create ns windows\n</code></pre> <p>Create a yaml file for the app from the configuration below and save it as windows-server-2022.yaml</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: windows-server-iis-ltsc2022\nnamespace: windows\nspec:\nselector:\nmatchLabels:\napp: windows-server-iis-ltsc2022\ntier: backend\ntrack: stable\nreplicas: 2\ntemplate:\nmetadata:\nlabels:\napp: windows-server-iis-ltsc2022\ntier: backend\ntrack: stable\nspec:\ncontainers:\n- name: windows-server-iis-ltsc2022\nimage: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2022\nports:\n- name: http\ncontainerPort: 80\nimagePullPolicy: IfNotPresent\ncommand:\n- powershell.exe\n- -command\n- \"Add-WindowsFeature Web-Server; Invoke-WebRequest -UseBasicParsing -Uri 'https://dotnetbinaries.blob.core.windows.net/servicemonitor/2.0.1.6/ServiceMonitor.exe' -OutFile 'C:\\\\ServiceMonitor.exe'; echo '&lt;html&gt;&lt;body&gt;&lt;br/&gt;&lt;br/&gt;&lt;H1&gt;Our first pods running on Windows managed node groups! Powered by Windows Server LTSC 2022.&lt;H1&gt;&lt;/body&gt;&lt;html&gt;' &gt; C:\\\\inetpub\\\\wwwroot\\\\iisstart.htm; C:\\\\ServiceMonitor.exe 'w3svc'; \"\nnodeSelector:\nkubernetes.io/os: windows\ntolerations:\n- key: \"os\"\noperator: \"Equal\"\nvalue: \"windows\"\neffect: \"NoSchedule\"\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: windows-server-iis-ltsc2022-service\nnamespace: windows\nspec:\nports:\n- port: 80\nprotocol: TCP\ntargetPort: 80\nselector:\napp: windows-server-iis-ltsc2022\ntier: backend\ntrack: stable\nsessionAffinity: None\ntype: LoadBalancer\n</code></pre> <p>Deploy the sample app</p> <pre><code>kubectl apply -f windows-server-2022.yaml\n</code></pre> <p>Verify the resources created successfully</p> <pre><code>kubectl get -n windows svc,deploy,pods\n</code></pre>"},{"location":"patterns/windows/#reference","title":"Reference","text":"<p>Please reference our blog on Deploying Amazon EKS Windows managed node groups to learn more about this topic.</p>"},{"location":"patterns/windows/#cleanup","title":"Cleanup","text":"<p>First delete the windows app</p> <pre><code>kubectl delete -f windows-server-2022.yaml\nkubectl delete ns windows\n</code></pre> <p>To clean up your EKS Blueprint, run the following command:</p> <pre><code>make pattern windows destroy\n</code></pre>"},{"location":"patterns/security/eks-config-rules/","title":"Security Best Practices for Amazon EKS","text":""},{"location":"patterns/security/eks-config-rules/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable AWS Config Managed Rules for EKS Security Best Practices to your AWS account, verify that it is enabled, and get findings from Security Hub.</p> <p>The pattern will enable Security Hub in the <code>CDK_DEFAULT_ACCOUNT</code> and <code>CDK_DEFAULT_REGION</code>, but only if it is not already enabled. If Security Hub is already enabled in the target AWS account and region the stack will fail and be rolled back.</p>"},{"location":"patterns/security/eks-config-rules/#prerequisites","title":"Prerequisites","text":"<ol> <li>Clone the repository.</li> <li>Follow the usage instructions to install the dependencies.</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> </ol>"},{"location":"patterns/security/eks-config-rules/#deploy","title":"Deploy","text":"<p>To update npm, run the following command:</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre>"},{"location":"patterns/security/eks-config-rules/#deploy-aws-config","title":"Deploy AWS Config","text":"<p>Use the AWS Config setup blueprints pattern enable AWS Config in your account and region by running the following command.</p> <pre><code>make pattern eks-config-rules deploy eks-config-setup\n</code></pre>"},{"location":"patterns/security/eks-config-rules/#deploy-config-rules-for-eks-security-best-practices","title":"Deploy Config Rules for EKS Security Best Practices","text":"<p>Now enable the AWS Config managed rules for EKS security best practices by running the following command.</p> <pre><code>make pattern eks-config-rules deploy eks-config-rules-setup\n</code></pre>"},{"location":"patterns/security/eks-config-rules/#verify","title":"Verify","text":""},{"location":"patterns/security/eks-config-rules/#verify-the-status-of-the-aws-config-managed-rules-for-eks-security-best-practices","title":"Verify the status of the AWS Config managed rules for EKS security best practices","text":"<p>Using the following AWS CLI command, get a list Config rules with their evaluation status.</p> <pre><code>aws configservice describe-config-rule-evaluation-status\n</code></pre> <p>The output will look something like the following.</p> <pre><code>{\n\"ConfigRulesEvaluationStatus\": [\n...\n{\n\"ConfigRuleName\": \"eks-config-rules-setup-EksEndpointNoPublicAccess49-37QJEXYZALLB\",\n\"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-luqz0p\",\n\"ConfigRuleId\": \"config-rule-luqz0p\",\n\"LastSuccessfulInvocationTime\": \"2023-05-30T00:33:26.878000+00:00\",\n\"LastSuccessfulEvaluationTime\": \"2023-05-30T00:33:27.539000+00:00\",\n\"FirstActivatedTime\": \"2023-05-27T00:32:41.020000+00:00\",\n\"FirstEvaluationStarted\": true\n},\n{\n\"ConfigRuleName\": \"eks-config-rules-setup-EksOldestSupportedVersionAD-Z65N0TEQSF96\",\n\"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-psbc54\",\n\"ConfigRuleId\": \"config-rule-psbc54\",\n\"LastSuccessfulInvocationTime\": \"2023-05-27T07:56:05.182000+00:00\",\n\"LastSuccessfulEvaluationTime\": \"2023-05-27T07:56:07.542000+00:00\",\n\"FirstActivatedTime\": \"2023-05-25T22:44:21.666000+00:00\",\n\"FirstEvaluationStarted\": true\n},\n{\n\"ConfigRuleName\": \"eks-config-rules-setup-EksSecretsEncrypted7566BFCD-HUQX4WXUDEFA\",\n\"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-kzohng\",\n\"ConfigRuleId\": \"config-rule-kzohng\",\n\"LastSuccessfulInvocationTime\": \"2023-05-30T00:33:26.902000+00:00\",\n\"LastSuccessfulEvaluationTime\": \"2023-05-30T00:33:27.616000+00:00\",\n\"FirstActivatedTime\": \"2023-05-27T00:32:41.006000+00:00\",\n\"FirstEvaluationStarted\": true\n},\n{\n\"ConfigRuleName\": \"eks-config-rules-setup-EksSupportedVersionCDB3159A-1VNH10LGMMJX\",\n\"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-oaio54\",\n\"ConfigRuleId\": \"config-rule-oaio54\",\n\"LastSuccessfulInvocationTime\": \"2023-05-27T07:56:05.223000+00:00\",\n\"LastSuccessfulEvaluationTime\": \"2023-05-27T07:56:05.420000+00:00\",\n\"FirstActivatedTime\": \"2023-05-25T22:51:26.563000+00:00\",\n\"FirstEvaluationStarted\": true\n}\n...\n]\n}\n</code></pre> <p>You can search for the EKS specific rules. Make a note of the unique <code>ConfigRuleName</code> of each of the Config rules for EKS security best practices.</p> <p>Using the unique names of the EKS Config rules from your account and region shown after running the previous AWS CLI command, you can verify each EKS Config rule configuration and state using the following AWS CLI command (Remember to replace the rule names below with your rule names).</p> <pre><code>aws configservice describe-config-rules --config-rule-names \"eks-config-rules-setup-EksEndpointNoPublicAccess&lt;your rule id&gt;\" \"eks-config-rules-setup-EksOldestSupportedVersion&lt;your rule id&gt;\" \"eks-config-rules-setup-EksSecretsEncrypted&lt;your rule id&gt;\" \"eks-config-rules-set\nup-EksSupportedVersion&lt;your rule id&gt;\"\n</code></pre> <pre><code>{\n\"ConfigRules\": [\n{\n\"ConfigRuleName\": \"eks-config-rules-setup-EksEndpointNoPublicAccess49-37QJEXYZALLB\",\n\"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-luqz0p\",\n\"ConfigRuleId\": \"config-rule-luqz0p\",\n\"Source\": {\n\"Owner\": \"AWS\",\n\"SourceIdentifier\": \"EKS_ENDPOINT_NO_PUBLIC_ACCESS\"\n},\n\"ConfigRuleState\": \"ACTIVE\",\n\"EvaluationModes\": [\n{\n\"Mode\": \"DETECTIVE\"\n}\n]\n},\n{\n\"ConfigRuleName\": \"eks-config-rules-setup-EksOldestSupportedVersionAD-Z65N0TEQSF96\",\n\"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-psbc54\",\n\"ConfigRuleId\": \"config-rule-psbc54\",\n\"Source\": {\n\"Owner\": \"AWS\",\n\"SourceIdentifier\": \"EKS_CLUSTER_OLDEST_SUPPORTED_VERSION\"\n},\n\"InputParameters\": \"{\\\"oldestVersionSupported\\\":\\\"1.25\\\"}\",\n\"ConfigRuleState\": \"ACTIVE\",\n\"EvaluationModes\": [\n{\n\"Mode\": \"DETECTIVE\"\n}\n]\n},\n{\n\"ConfigRuleName\": \"eks-config-rules-setup-EksSecretsEncrypted7566BFCD-HUQX4WXUDEFA\",\n\"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-kzohng\",\n\"ConfigRuleId\": \"config-rule-kzohng\",\n\"Source\": {\n\"Owner\": \"AWS\",\n\"SourceIdentifier\": \"EKS_SECRETS_ENCRYPTED\"\n},\n\"ConfigRuleState\": \"ACTIVE\",\n\"EvaluationModes\": [\n{\n\"Mode\": \"DETECTIVE\"\n}\n]\n},\n{\n\"ConfigRuleName\": \"eks-config-rules-setup-EksSupportedVersionCDB3159A-1VNH10LGMMJX\",\n\"ConfigRuleArn\": \"arn:aws:config:us-east-1:XXXXXXXXXXX:config-rule/config-rule-oaio54\",\n\"ConfigRuleId\": \"config-rule-oaio54\",\n\"Source\": {\n\"Owner\": \"AWS\",\n\"SourceIdentifier\": \"EKS_CLUSTER_SUPPORTED_VERSION\"\n},\n\"InputParameters\": \"{\\\"oldestVersionSupported\\\":\\\"1.25\\\"}\",\n\"ConfigRuleState\": \"ACTIVE\",\n\"EvaluationModes\": [\n{\n\"Mode\": \"DETECTIVE\"\n}\n]\n}\n]\n}\n</code></pre> <p>Note that you can see the parameter value of the rules with required <code>InputParameters</code> (<code>EKS_CLUSTER_OLDEST_SUPPORTED_VERSION</code> and <code>EKS_CLUSTER_OLDEST_SUPPORTED_VERSION</code>), and the <code>ConfigRuleState</code> for each of the rules which is <code>ACTIVE</code>.</p>"},{"location":"patterns/security/encryption-at-rest/","title":"EKS Encryption-at-Rest pattern","text":""},{"location":"patterns/security/encryption-at-rest/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable encryption at rest for EKS cluster using EBS/EFS storage.</p> <p>To achieve this objective, the pattern utilizes EBS CSI Driver Amazon EKS Add-on to enable encryption-at-rest for EBS volumes. The pattern also leverages EFS CSI Driver Amazon EKS Add-on to enable encryption-at-rest for EFS volumes.</p> <p>The pattern also leverages KMS resource provider to create KMS keys for EBS/EFS encryption-at-rest and EFS File System resource provider to create an encrypted EFS file system.</p>"},{"location":"patterns/security/encryption-at-rest/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstraps the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p> <p>The sample repository contains the following workloads:</p> <ol> <li>team-platform creates a storage class for EBS and EFS volumes.</li> <li>team-data creates a persistent volume claim for EBS and EFS volumes and a pod that mounts the volumes.</li> </ol>"},{"location":"patterns/security/encryption-at-rest/#prerequisites","title":"Prerequisites","text":"<ol> <li>Clone the repository.</li> <li>Follow the usage instructions to install the dependencies.</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> </ol>"},{"location":"patterns/security/encryption-at-rest/#deploy","title":"Deploy","text":"<p>To update npm, run the following command:</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre> <p>To deploy the pattern, run the following command:</p> <pre><code>make pattern data-at-rest-encryption deploy\n</code></pre>"},{"location":"patterns/security/encryption-at-rest/#verify","title":"Verify","text":"<p>Now you can verify that the EBS and EFS volumes are encrypted.</p>"},{"location":"patterns/security/encryption-at-rest/#ebs","title":"EBS","text":"<p>To list all the PersistentVolumeClaims (PVCs) that exist in the Kubernetes cluster's namespace named \"data\", run the following command:</p> <pre><code>kubectl get pvc -n data\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE\ngp2-encrypted-pvc   Bound    pvc-78bd070e-8eba-4b01-a378-462bb806beb3   10Gi       RWO            gp2-encrypted   14m\n</code></pre> <p>To describe an Amazon Elastic Block Store (EBS) volume that is associated with a PersistentVolume (PV) in Kubernetes, run the following command (please replace the PVC-IDENTIFIER with the PVC name from the previous step):</p> <pre><code>aws ec2 describe-volumes --region us-east-1 --filters \"Name=tag:kubernetes.io/created-for/pv/name,Values=&lt;PVC-IDENTIFIER&gt;\" --query 'Volumes[*].{VolumeId:VolumeId, Encrypted:Encrypted, KmsKeyId:KmsKeyId}'\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>[\n{\n\"VolumeId\": \"vol-09332f96a58e67385\",\n        \"Encrypted\": true,\n        \"KmsKeyId\": \"arn:aws:kms:us-east-1:111122223333:key/a8b9fa0b-955f-4f85-85c1-8f911003390e\"\n}\n]\n</code></pre>"},{"location":"patterns/security/encryption-at-rest/#efs","title":"EFS","text":"<p>To list all the StorageClasses that are defined in the Kubernetes cluster, run the following command:</p> <pre><code>kubectl get storageclass\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>NAME                      PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nefs-encrypted (default)   efs.csi.aws.com         Delete          Immediate              false                  70m\n</code></pre> <p>To retrieve the KMS Key ID parameter of a specific StorageClass named \"efs-encrypted\" in the Kubernetes cluster, run the following command:</p> <pre><code>kubectl get storageclass efs-encrypted -o jsonpath='{.parameters.kmsKeyId}'\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>arn:aws:kms:us-east-1:111222333444:key/19f4f602-dcf3-42a5-8eef-38f2af4b3626%  </code></pre> <p>To list all the PersistentVolumeClaims (PVCs) that exist in the Kubernetes cluster's namespace named \"data\", run the following command:</p> <pre><code>kubectl get pvc -n data\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>NAME                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE\nefs-encrypted-claim   Bound    pvc-06df2640-ae2f-44ae-8d5c-82c72e56a9ae   10Gi       RWX            efs-encrypted   63m\n</code></pre> <p>To list all the pods that are running in the Kubernetes cluster's namespace named \"data\", run the following command:</p> <pre><code>kubectl get pods -n data\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>NAME                 READY   STATUS    RESTARTS   AGE\nefs-encryption-app   1/1     Running   0          63m\n</code></pre> <p>To get detailed information about a PersistentVolumeClaim (PVC) named \"efs-encrypted-claim\" in the \"data\" namespace of the Kubernetes cluster, run the following command:</p> <pre><code>kubectl describe pvc efs-encrypted-claim -n data\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>Name:          efs-encrypted-claim\nNamespace:     data\nStorageClass:  efs-encrypted\nStatus:        Bound\nVolume:        pvc-06df2640-ae2f-44ae-8d5c-82c72e56a9ae\nLabels:        argocd.argoproj.io/instance=team-data\nAnnotations:   pv.kubernetes.io/bind-completed: yes\n               pv.kubernetes.io/bound-by-controller: yes\n               volume.beta.kubernetes.io/storage-provisioner: efs.csi.aws.com\n               volume.kubernetes.io/storage-provisioner: efs.csi.aws.com\nFinalizers:    [kubernetes.io/pvc-protection]\nCapacity:      10Gi\nAccess Modes:  RWX\nVolumeMode:    Filesystem\nUsed By:       efs-encryption-app\nEvents:        &lt;none&gt;\n</code></pre>"},{"location":"patterns/security/guardduty/","title":"Amazon GuardDuty Protection","text":""},{"location":"patterns/security/guardduty/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable Amazon GuardDuty Detector across your AWS accounts, use GuardDuty optional features, and how to automate notifications via Amazon SNS based on security vulnerabilities triggered by Amazon GuardDuty.</p> <p>Supported features:</p> <ul> <li>Foundational data sources - these data sources are enabled by default, no need to mention them in the pattern input</li> <li>EKS Audit Log Monitoring</li> <li>EKS Runtime Monitoring</li> <li>Malware Protection in Amazon GuardDuty</li> <li>GuardDuty RDS Protection</li> <li>Amazon S3 Protection in Amazon GuardDuty</li> </ul> <p>The pattern consists of two components:</p> <ul> <li><code>GuardDutySetupStack</code> - enables GuardDuty Detector for the account. The stack also creates an SNS topic, SNS Subscription, and Amazon EventBridge Rule.</li> <li>A blueprint that deploys a sample GitOps workload that triggers a GuardDuty finding.</li> </ul> <p>The list of optional features is adjustable via the <code>features</code> parameter in the GuardDutySetupStack stack.</p>"},{"location":"patterns/security/guardduty/#gitops-configuration","title":"GitOps configuration","text":"<p>For GitOps, the blueprint bootstraps the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p> <p>The sample repository contains the following workloads:</p> <ul> <li><code>team-danger</code> runs a pod in a privileged mode which is a security anti-pattern</li> <li><code>team-danger</code> runs a pod with a malicious file</li> </ul>"},{"location":"patterns/security/guardduty/#prerequisites","title":"Prerequisites","text":"<ol> <li>Clone the repository</li> <li>Follow the usage instructions to install the dependencies</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> </ol>"},{"location":"patterns/security/guardduty/#deploy","title":"Deploy","text":"<p>To update npm, run the following command:</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre>"},{"location":"patterns/security/guardduty/#deploying-the-guarddutysetupstack-stack","title":"Deploying the <code>GuardDutySetupStack</code> stack","text":"<p>The <code>GuardDutySetupStack</code> stack enables GuardDuty Detector for the account with all the features of your choice enabled. For the purposes of the Security seciton of this workshop only the GuardDuty EKS Protection features are required.</p> <p></p> <p>Note: You can only deploy this stack if you have not already enabled GuardDuty in the target account and region. If GuardDuty has been enabled already, do not attempt to deploy the stack as GuardDuty can only be enabled once per account and region. Instead, check that the EKS Protection features have been enabled either in the AWS GuardDuty console as shown in the image above, or using the AWS CLI, then proceed to Deploying the blueprint workload step.</p> <p>To deploy the stack, run the following command:</p> <pre><code>make pattern guardduty deploy guardduty-setup\n</code></pre>"},{"location":"patterns/security/guardduty/#deploying-the-blueprint-workload","title":"Deploying the blueprint workload","text":"<p>The blueprint deploys a sample GitOps workload that triggers a GuardDuty finding.</p> <p>To deploy the blueprint, run the following command:</p> <pre><code>make pattern guardduty deploy guardduty-blueprint\n</code></pre>"},{"location":"patterns/security/guardduty/#verify","title":"Verify","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found here. Please replace <code>&lt;your cluster name&gt;</code>, <code>&lt;your region&gt;</code>, and <code>&lt;your cluster role arn&gt;</code> with the values from the CDK output message.</p> <pre><code>aws eks update-kubeconfig --name &lt;your cluster name&gt; --region &lt;your region&gt; --role-arn &lt;your cluster role arn&gt;\n</code></pre>"},{"location":"patterns/security/guardduty/#verifying-that-the-guardduty-detector-is-enabled","title":"Verifying that the GuardDuty detector is enabled","text":"<p>Now you can check that the GuardDuty detector is successfully enabled with all the required data sources.</p> <p>To list all detectors in the region, run the following command:</p> <pre><code>aws guardduty list-detectors --region us-east-1\n</code></pre> <p>The output should look like this:</p> <pre><code>{\n\"DetectorIds\": [\n\"80c3c03d44819a984b035b000aa9b3da\"\n]\n}\n</code></pre> <p>To check the detector's configuration, run the following command (please replace <code>&lt;DETECTOR-ID&gt;</code> with the ID of the detector):</p> <pre><code>aws guardduty get-detector --detector-id &lt;DETECTOR-ID&gt; --region us-east-1\n</code></pre> <p>The output should look like this:</p> <pre><code>{\n\"CreatedAt\": \"2023-04-14T15:55:27.088Z\",\n\"FindingPublishingFrequency\": \"SIX_HOURS\",\n\"ServiceRole\": \"arn:aws:iam::123456789012:role/aws-service-role/guardduty.amazonaws.com/AWSServiceRoleForAmazonGuardDuty\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T15:55:27.088Z\",\n\"DataSources\": {\n\"CloudTrail\": {\n\"Status\": \"ENABLED\"\n},\n\"DNSLogs\": {\n\"Status\": \"ENABLED\"\n},\n\"FlowLogs\": {\n\"Status\": \"ENABLED\"\n},\n\"S3Logs\": {\n\"Status\": \"ENABLED\"\n},\n\"Kubernetes\": {\n\"AuditLogs\": {\n\"Status\": \"ENABLED\"\n}\n},\n\"MalwareProtection\": {\n\"ScanEc2InstanceWithFindings\": {\n\"EbsVolumes\": {\n\"Status\": \"ENABLED\"\n}\n},\n\"ServiceRole\": \"arn:aws:iam::123456789012:role/aws-service-role/malware-protection.guardduty.amazonaws.com/AWSServiceRoleForAmazonGuardDutyMalwareProtection\"\n}\n},\n\"Tags\": {},\n\"Features\": [\n{\n\"Name\": \"CLOUD_TRAIL\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T11:08:44-05:00\"\n},\n{\n\"Name\": \"DNS_LOGS\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T11:08:44-05:00\"\n},\n{\n\"Name\": \"FLOW_LOGS\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T11:08:44-05:00\"\n},\n{\n\"Name\": \"S3_DATA_EVENTS\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n},\n{\n\"Name\": \"EKS_AUDIT_LOGS\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n},\n{\n\"Name\": \"EBS_MALWARE_PROTECTION\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n},\n{\n\"Name\": \"RDS_LOGIN_EVENTS\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n},\n{\n\"Name\": \"EKS_RUNTIME_MONITORING\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T10:55:27-05:00\",\n\"AdditionalConfiguration\": [\n{\n\"Name\": \"EKS_ADDON_MANAGEMENT\",\n\"Status\": \"ENABLED\",\n\"UpdatedAt\": \"2023-04-14T10:55:27-05:00\"\n}\n]\n}\n]\n}\n```\n\n### Verifying that the GuardDuty findings are generated\n\nTo list all findings in the region, run the following command (please replace `&lt;DETECTOR-ID&gt;` with the ID of the detector):\n\n```bash\naws guardduty list-findings --detector-id &lt;DETECTOR-ID&gt; --region us-east-1\n</code></pre> <p>The output should look like this:</p> <pre><code>{\n\"FindingIds\": [\n\"f2c3859c6ca25b3057d13470a992bbd7\"\n]\n}\n</code></pre> <p>To check the finding's details, run the following command (please replace <code>&lt;DETECTOR-ID&gt;</code> and <code>&lt;FINDING-ID&gt;</code> with the ID of the detector and the ID of the finding):</p> <pre><code>aws guardduty get-findings --detector-id &lt;DETECTOR-ID&gt; --finding-ids &lt;FINDING-ID&gt; --region us-east-1\n</code></pre> <p>The list of findings contains <code>PrivilegeEscalation:Kubernetes/PrivilegedContainer</code> as expected:</p> <pre><code>{\n\"Findings\": [\n{\n\"AccountId\": \"123456789012\",\n\"Arn\": \"arn:aws:guardduty:us-east-1:123456789012:detector/94c3858788bc1444ceedab472bab5d7e/finding/f2c3859c6ca25b3057d13470a992bbd7\",\n\"CreatedAt\": \"2023-03-22T21:28:07.748Z\",\n\"Description\": \"A privileged container with root level access was launched on EKS Cluster guardduty-blueprint. If this behavior is not expected, it may indicate that your credentials are compromised.\",\n\"Id\": \"f2c3859c6ca25b3057d13470a992bbd7\",\n\"Partition\": \"aws\",\n\"Region\": \"us-east-1\",\n\"Resource\": {\n\"EksClusterDetails\": {\n\"Name\": \"guardduty-blueprint\",\n\"Arn\": \"arn:aws:eks:us-east-1:123456789012:cluster/guardduty-blueprint\",\n\"VpcId\": \"vpc-02b68c9ddc1d403ab\",\n\"Status\": \"ACTIVE\",\n\"Tags\": [],\n\"CreatedAt\": \"2023-03-22T15:48:25.752000-05:00\"\n},\n\"KubernetesDetails\": {\n\"KubernetesUserDetails\": {\n\"Username\": \"system:serviceaccount:argocd:argocd-application-controller\",\n\"Uid\": \"1871d525-442e-487f-ae60-81336d1ff0cf\",\n\"Groups\": [\n\"system:serviceaccounts\",\n\"system:serviceaccounts:argocd\",\n\"system:authenticated\"\n]\n},\n\"KubernetesWorkloadDetails\": {\n\"Name\": \"privileged-pod\",\n\"Type\": \"pods\",\n\"Uid\": \"33a3c89e-3280-474d-b8cb-fdf03394fc15\",\n\"Namespace\": \"argocd\",\n\"HostNetwork\": false,\n\"Containers\": [\n{\n\"Name\": \"app\",\n\"Image\": \"centos\",\n\"ImagePrefix\": \"\",\n\"SecurityContext\": {\n\"Privileged\": true\n}\n}\n]\n}\n},\n\"ResourceType\": \"EKSCluster\"\n},\n\"SchemaVersion\": \"2.0\",\n\"Service\": {\n\"Action\": {\n\"ActionType\": \"KUBERNETES_API_CALL\",\n\"KubernetesApiCallAction\": {\n\"RequestUri\": \"/api/v1/namespaces/argocd/pods\",\n\"Verb\": \"create\",\n\"UserAgent\": \"argocd-application-controller/v0.0.0 (linux/amd64) kubernetes/$Format\",\n\"RemoteIpDetails\": {\n\"City\": {\n\"CityName\": \"UNKNOWN\"\n},\n\"Country\": {},\n\"GeoLocation\": {\n\"Lat\": 0.0,\n\"Lon\": 0.0\n},\n\"IpAddressV4\": \"10.0.205.129\",\n\"Organization\": {\n\"Asn\": \"0\",\n\"AsnOrg\": \"UNKNOWN\",\n\"Isp\": \"UNKNOWN\",\n\"Org\": \"UNKNOWN\"\n}\n},\n\"StatusCode\": 201\n}\n},\n\"Archived\": false,\n\"Count\": 1,\n\"DetectorId\": \"94c3858788bc1444ceedab472bab5d7e\",\n\"EventFirstSeen\": \"2023-03-22T21:27:18.186Z\",\n\"EventLastSeen\": \"2023-03-22T21:27:18.630Z\",\n\"ResourceRole\": \"TARGET\",\n\"ServiceName\": \"guardduty\",\n\"AdditionalInfo\": {\n\"Value\": \"{}\",\n\"Type\": \"default\"\n}\n},\n\"Severity\": 5,\n\"Title\": \"Privileged container with root level access launched on the EKS Cluster.\",\n\"Type\": \"PrivilegeEscalation:Kubernetes/PrivilegedContainer\",\n\"UpdatedAt\": \"2023-03-22T21:28:07.748Z\"\n}\n]\n}\n</code></pre>"},{"location":"patterns/security/guardduty/#verifying-that-the-guardduty-runtime-monitoring-agents-are-automatically-deployed","title":"Verifying that the GuardDuty Runtime Monitoring agents are automatically deployed","text":"<p>To verify that the GuardDuty Runtime Monitoring agents are automatically deployed, run the following command:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>The output should look like this:</p> <pre><code>NAMESPACE          NAME                                                              READY   STATUS    RESTARTS        AGE\namazon-guardduty   aws-guardduty-agent-qrm22                                         1/1     Running   0               25m\nargocd             blueprints-addon-argocd-application-controller-0                  1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-applicationset-controller-7c4c75877579s   1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-dex-server-c6687d84f-q4697                1/1     Running   1 (3m21s ago)   3m25s\nargocd             blueprints-addon-argocd-notifications-controller-7c74f76c5wh4nb   1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-redis-595cc69fff-9985j                    1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-repo-server-7f75c7796c-229c4              1/1     Running   0               3m25s\nargocd             blueprints-addon-argocd-server-86867c9dd8-p6qk7                   1/1     Running   0               3m25s\nargocd             privileged-pod                                                    1/1     Running   0               115s\nkube-system        aws-node-4lhp7                                                    1/1     Running   0               26m\nkube-system        coredns-79989457d9-jncrb                                          1/1     Running   0               32m\nkube-system        coredns-79989457d9-l5jcg                                          1/1     Running   0               32m\nkube-system        kube-proxy-hwkwm                                                  1/1     Running   0               26m\n</code></pre> <p>As you can see, the GuardDuty Runtime Monitoring agent is deployed in the <code>amazon-guardduty</code> namespace.</p>"},{"location":"patterns/security/image-scanning/","title":"Amazon ECR Image Scanning","text":""},{"location":"patterns/security/image-scanning/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable and configure Amazon ECR image scanning.</p> <p>The following scanning types are offered:</p> <ul> <li>Enhanced scanning\u2014Amazon ECR integrates with Amazon Inspector to provide automated, continuous scanning of your repositories. Your container images are scanned for both operating systems and programing language package vulnerabilities. As new vulnerabilities appear, the scan results are updated and Amazon Inspector emits an event to EventBridge to notify you.</li> <li>Basic scanning\u2014Amazon ECR uses the Common Vulnerabilities and Exposures (CVEs) database from the open-source Clair project. With basic scanning, you configure your repositories to scan on push or you can perform manual scans and Amazon ECR provides a list of scan findings.</li> </ul> <p>The pattern consists of two components:</p> <ul> <li><code>ImageScanningSetupStack</code> - configures the Amazon ECR image scanning and the ECR automated re-scan duration in Inspector.</li> <li>A blueprint that deploys a sample GitOps workload that pushes images to Amazon ECR and triggers the image scanning.</li> </ul>"},{"location":"patterns/security/image-scanning/#configuration","title":"Configuration","text":"<p>You can configure the following parameters in the ImageScanningSetupStack stack:</p> <ul> <li><code>scanType</code> - The type of scan to perform. Valid values are <code>BASIC</code> and <code>ENHANCED</code>.</li> <li>Enhanced scanning only:</li> <li><code>enhancedContinuousScanDuration</code> - the Amazon ECR automated re-scan duration setting determines how long Amazon Inspector continuously monitors images pushed into repositories. When the number of days from when an image is first pushed exceeds the automated re-scan duration configuration, Amazon Inspector stops monitoring the image. When Amazon Inspector stops monitoring an image, the scan status of the image is changed to inactive with a reason code of expired, and all associated findings for the image are scheduled to be closed. Valid values are <code>LIFETIME</code>, <code>DAYS_30</code>, and <code>DAYS_180</code>.</li> <li><code>enhancedScanRules</code> - the scanning rules.</li> <li>Basic scanning only:</li> <li><code>basicScanRules</code> - the scanning rules.</li> </ul> <p>Please refer to the Amazon ECR Image Scanning documentation for more information and how to use filters.</p>"},{"location":"patterns/security/image-scanning/#gitops-confguration","title":"GitOps confguration","text":"<p>For GitOps, the blueprint bootstraps the ArgoCD addon and points to the EKS Blueprints Workload sample repository.</p> <p>The sample repository contains the following workloads:</p> <ul> <li><code>team-scan</code> pushes a Docker image to Amazon ECR and triggers the image scanning.</li> </ul>"},{"location":"patterns/security/image-scanning/#prerequisites","title":"Prerequisites","text":"<ol> <li>Clone the repository</li> <li>Follow the usage instructions to install the dependencies</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> </ol>"},{"location":"patterns/security/image-scanning/#deploy","title":"Deploy","text":"<p>To update npm, run the following command:</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre>"},{"location":"patterns/security/image-scanning/#deploying-the-imagescanningsetupstack-stack","title":"Deploying the <code>ImageScanningSetupStack</code> stack","text":"<p>The <code>ImageScanningSetupStack</code> configures the Amazon ECR image scanning and the ECR automated re-scan duration in Inspector.</p> <p>To deploy the stack, run the following command:</p> <pre><code>make pattern ecr-image-scanning deploy image-scanning-setup\n</code></pre>"},{"location":"patterns/security/image-scanning/#deploying-the-blueprint","title":"Deploying the blueprint","text":"<p>The blueprint deploys a sample GitOps workload that pushes images to Amazon ECR and triggers the image scanning.</p> <p>To deploy the blueprint, run the following command:</p> <pre><code>make pattern ecr-image-scanning deploy image-scanning-workload-blueprint\n</code></pre>"},{"location":"patterns/security/image-scanning/#verify","title":"Verify","text":""},{"location":"patterns/security/image-scanning/#verifying-that-the-image-scanning-is-enabled","title":"Verifying that the image scanning is enabled","text":"<p>To verify that the image scanning is enabled at the registry level, run the following command:</p> <pre><code>aws ecr get-registry-scanning-configuration\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>{\n\"registryId\": \"123456789012\",\n\"scanningConfiguration\": {\n\"scanType\": \"ENHANCED\",\n\"rules\": [\n{\n\"scanFrequency\": \"CONTINUOUS_SCAN\",\n\"repositoryFilters\": [\n{\n\"filter\": \"prod\",\n\"filterType\": \"WILDCARD\"\n}\n]\n},\n{\n\"scanFrequency\": \"SCAN_ON_PUSH\",\n\"repositoryFilters\": [\n{\n\"filter\": \"*\",\n\"filterType\": \"WILDCARD\"\n}\n]\n}\n]\n}\n}\n</code></pre>"},{"location":"patterns/security/image-scanning/#verifying-that-the-image-is-pushed-to-amazon-ecr","title":"Verifying that the image is pushed to Amazon ECR","text":"<p>To verify that the image is pushed to Amazon ECR, run the following command (please replace <code>&lt;REPOSITORY-NAME&gt;</code> with the repository name):</p> <pre><code>aws ecr describe-images --repository-name &lt;REPOSITORY-NAME&gt;\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>{\n\"imageDetails\": [\n{\n\"registryId\": \"123456789012\",\n\"repositoryName\": \"image-scanning-workload-blueprint-imagescanningrepository754c6116-arh0wk3afnkw\",\n\"imageDigest\": \"sha256:a1801b843b1bfaf77c501e7a6d3f709401a1e0c83863037fa3aab063a7fdb9dc\",\n\"imageTags\": [\n\"latest\"\n],\n\"imageSizeInBytes\": 83520228,\n\"imagePushedAt\": \"2023-04-17T17:22:33-05:00\",\n\"imageManifestMediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n\"artifactMediaType\": \"application/vnd.docker.container.image.v1+json\",\n\"lastRecordedPullTime\": \"2023-04-17T17:22:33.966000-05:00\"\n}\n]\n}\n</code></pre>"},{"location":"patterns/security/image-scanning/#checking-the-image-scanning-findings","title":"Checking the image scanning findings","text":"<p>To check the image scanning findings, run the following command (please replace <code>&lt;REPOSITORY-NAME&gt;</code> with the repository name):</p> <pre><code>aws ecr describe-image-scan-findings --repository-name &lt;REPOSITORY-NAME&gt; --image-id imageTag=latest\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>{\n\"imageScanFindings\": {\n\"enhancedFindings\": [\n{\n\"awsAccountId\": \"123456789012\",\n\"description\": \"basic/unit-name.c in systemd prior to 246.15, 247.8, 248.5, and 249.1 has a Memory Allocation with an Excessive Size Value (involving strdupa and alloca for a pathname controlled by a local attacker) that results in an operating system crash.\",\n\"findingArn\": \"arn:aws:inspector2:us-east-1:123456789012:finding/0407d7719da0fc8a8f44991f0bf524d6\",\n\"firstObservedAt\": \"2023-04-17T17:40:39.940000-05:00\",\n\"lastObservedAt\": \"2023-04-17T17:40:39.940000-05:00\",\n\"packageVulnerabilityDetails\": {\n\"cvss\": [\n{\n\"baseScore\": 4.9,\n\"scoringVector\": \"AV:L/AC:L/Au:N/C:N/I:N/A:C\",\n\"source\": \"NVD\",\n\"version\": \"2.0\"\n},\n{\n\"baseScore\": 5.5,\n\"scoringVector\": \"CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H\",\n\"source\": \"NVD\",\n\"version\": \"3.1\"\n}\n],\n\"referenceUrls\": [\n\"https://www.debian.org/security/2021/dsa-4942\",\n\"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/42TMJVNYRY65B4QCJICBYOEIVZV3KUYI/\",\n\"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/2LSDMHAKI4LGFOCSPXNVVSEWQFAVFWR7/\",\n\"https://security.gentoo.org/glsa/202107-48\",\n\"https://cert-portal.siemens.com/productcert/pdf/ssa-222547.pdf\"\n],\n\"relatedVulnerabilities\": [],\n\"source\": \"NVD\",\n\"sourceUrl\": \"https://nvd.nist.gov/vuln/detail/CVE-2021-33910\",\n\"vendorCreatedAt\": \"2021-07-20T14:15:00-05:00\",\n\"vendorSeverity\": \"MEDIUM\",\n\"vendorUpdatedAt\": \"2022-06-14T06:15:00-05:00\",\n\"vulnerabilityId\": \"CVE-2021-33910\",\n\"vulnerablePackages\": [\n{\n\"arch\": \"X86_64\",\n\"epoch\": 0,\n\"name\": \"systemd-pam\",\n\"packageManager\": \"OS\",\n\"release\": \"45.el8\",\n\"sourceLayerHash\": \"sha256:a1d0c75327776413fa0db9ed3adcdbadedc95a662eb1d360dad82bb913f8a1d1\",\n\"version\": \"239\"\n},\n{\n\"arch\": \"X86_64\",\n\"epoch\": 0,\n\"name\": \"systemd\",\n\"packageManager\": \"OS\",\n\"release\": \"45.el8\",\n\"sourceLayerHash\": \"sha256:a1d0c75327776413fa0db9ed3adcdbadedc95a662eb1d360dad82bb913f8a1d1\",\n\"version\": \"239\"\n},\n{\n\"arch\": \"X86_64\",\n\"epoch\": 0,\n\"name\": \"systemd-libs\",\n\"packageManager\": \"OS\",\n\"release\": \"45.el8\",\n\"sourceLayerHash\": \"sha256:a1d0c75327776413fa0db9ed3adcdbadedc95a662eb1d360dad82bb913f8a1d1\",\n\"version\": \"239\"\n},\n{\n\"arch\": \"X86_64\",\n\"epoch\": 0,\n\"name\": \"systemd-udev\",\n\"packageManager\": \"OS\",\n\"release\": \"45.el8\",\n\"sourceLayerHash\": \"sha256:a1d0c75327776413fa0db9ed3adcdbadedc95a662eb1d360dad82bb913f8a1d1\",\n\"version\": \"239\"\n}\n]\n},\n\"remediation\": {\n\"recommendation\": {\n\"text\": \"None Provided\"\n}\n},\n\"resources\": [\n{\n\"details\": {\n\"awsEcrContainerImage\": {\n\"architecture\": \"amd64\",\n\"imageHash\": \"sha256:a1801b843b1bfaf77c501e7a6d3f709401a1e0c83863037fa3aab063a7fdb9dc\",\n\"imageTags\": [\n\"latest\"\n],\n\"platform\": \"CENTOS_8\",\n\"pushedAt\": \"2023-04-17T17:22:33-05:00\",\n\"registry\": \"123456789012\",\n\"repositoryName\": \"image-scanning-workload-blueprint-imagescanningrepository754c6116-arh0wk3afnkw\"\n}\n},\n\"id\": \"arn:aws:ecr:us-east-1:123456789012:repository/image-scanning-workload-blueprint-imagescanningrepository754c6116-arh0wk3afnkw/sha256:a1801b843b1bfaf77c501e7a6d3f709401a1e0c83863037fa3aab063a7fdb9dc\",\n\"tags\": {},\n\"type\": \"AWS_ECR_CONTAINER_IMAGE\"\n}\n],\n\"score\": 5.5,\n\"scoreDetails\": {\n\"cvss\": {\n\"adjustments\": [],\n\"score\": 5.5,\n\"scoreSource\": \"NVD\",\n\"scoringVector\": \"CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H\",\n\"version\": \"3.1\"\n}\n},\n\"severity\": \"MEDIUM\",\n\"status\": \"ACTIVE\",\n\"title\": \"CVE-2021-33910 - systemd-pam, systemd and 2 more\",\n\"type\": \"PACKAGE_VULNERABILITY\",\n\"updatedAt\": \"2023-04-17T17:40:39.940000-05:00\"\n},\n}\n...            </code></pre> <p>You can also check the findings in Inspector2.</p> <pre><code>aws inspector2 list-findings\n</code></pre> <p>The output should look similar to the following:</p> <pre><code>{\n\"findings\": [\n{\n\"awsAccountId\": \"123456789012\",\n\"description\": \"When curl is instructed to get content using the metalink feature, and a user name and password are used to download the metalink XML file, those same credentials are then subsequently passed on to each of the servers from which curl will download or try to download the contents from. Often contrary to the user's expectations and intentions and without telling the user it happened.\",\n\"exploitAvailable\": \"NO\",\n\"findingArn\": \"arn:aws:inspector2:us-east-1:123456789012:finding/006e8eac196bf27417099413ce74eb1a\",\n\"firstObservedAt\": \"2023-04-14T21:03:02.932000-05:00\",\n\"fixAvailable\": \"YES\",\n\"inspectorScore\": 5.3,\n\"inspectorScoreDetails\": {\n\"adjustedCvss\": {\n\"adjustments\": [],\n\"cvssSource\": \"NVD\",\n\"score\": 5.3,\n\"scoreSource\": \"NVD\",\n\"scoringVector\": \"CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:N/A:N\",\n\"version\": \"3.1\"\n}\n},\n\"lastObservedAt\": \"2023-04-17T13:34:41.687000-05:00\",\n\"packageVulnerabilityDetails\": {\n\"cvss\": [\n{\n\"baseScore\": 2.6,\n\"scoringVector\": \"AV:N/AC:H/Au:N/C:P/I:N/A:N\",\n\"source\": \"NVD\",\n\"version\": \"2.0\"\n},\n{\n\"baseScore\": 5.3,\n\"scoringVector\": \"CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:H/I:N/A:N\",\n\"source\": \"NVD\",\n\"version\": \"3.1\"\n}\n],\n\"referenceUrls\": [\n\"https://hackerone.com/reports/1213181\",\n\"https://security.gentoo.org/glsa/202212-01\",\n\"https://cert-portal.siemens.com/productcert/pdf/ssa-389290.pdf\",\n\"https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/FRUCW2UVNYUDZF72DQLFQR4PJEC6CF7V/\",\n\"https://www.oracle.com/security-alerts/cpuoct2021.html\"\n],\n\"relatedVulnerabilities\": [],\n\"source\": \"NVD\",\n\"sourceUrl\": \"https://nvd.nist.gov/vuln/detail/CVE-2021-22923\",\n\"vendorCreatedAt\": \"2021-08-05T16:15:00-05:00\",\n\"vendorSeverity\": \"MEDIUM\",\n\"vendorUpdatedAt\": \"2023-01-05T12:17:00-06:00\",\n\"vulnerabilityId\": \"CVE-2021-22923\",\n\"vulnerablePackages\": [\n{\n\"arch\": \"AARCH64\",\n\"epoch\": 0,\n\"fixedInVersion\": \"0:7.61.1-18.el8_4.1\",\n\"name\": \"curl\",\n\"packageManager\": \"OS\",\n\"release\": \"18.el8\",\n\"remediation\": \"dnf update curl\",\n\"sourceLayerHash\": \"sha256:52f9ef134af7dd14738733e567402af86136287d9468978d044780a6435a1193\",\n\"version\": \"7.61.1\"\n},\n{\n\"arch\": \"AARCH64\",\n\"epoch\": 0,\n\"fixedInVersion\": \"0:7.61.1-18.el8_4.1\",\n\"name\": \"libcurl-minimal\",\n\"packageManager\": \"OS\",\n\"release\": \"18.el8\",\n\"remediation\": \"dnf update libcurl-minimal\",\n\"sourceLayerHash\": \"sha256:52f9ef134af7dd14738733e567402af86136287d9468978d044780a6435a1193\",\n\"version\": \"7.61.1\"\n}\n]\n},\n\"remediation\": {\n\"recommendation\": {\n\"text\": \"None Provided\"\n}\n},\n\"resources\": [\n{\n\"details\": {\n\"awsEcrContainerImage\": {\n\"architecture\": \"arm64\",\n\"imageHash\": \"sha256:65a4aad1156d8a0679537cb78519a17eb7142e05a968b26a5361153006224fdc\",\n\"imageTags\": [\n\"latest\"\n],\n\"platform\": \"CENTOS_8\",\n\"pushedAt\": \"2023-04-17T13:34:34-05:00\",\n\"registry\": \"123456789012\",\n\"repositoryName\": \"cdk-hnb659fds-container-assets-123456789012-us-east-1\"\n}\n},\n\"id\": \"arn:aws:ecr:us-east-1:123456789012:repository/cdk-hnb659fds-container-assets-123456789012-us-east-1/sha256:65a4aad1156d8a0679537cb78519a17eb7142e05a968b26a5361153006224fdc\",\n\"partition\": \"aws\",\n\"region\": \"us-east-1\",\n\"tags\": {},\n\"type\": \"AWS_ECR_CONTAINER_IMAGE\"\n}\n],\n\"severity\": \"MEDIUM\",\n\"status\": \"CLOSED\",\n\"title\": \"CVE-2021-22923 - curl, libcurl-minimal\",\n\"type\": \"PACKAGE_VULNERABILITY\",\n\"updatedAt\": \"2023-04-17T13:36:44.258000-05:00\"\n},\n...\n</code></pre>"},{"location":"patterns/security/securityhub/","title":"AWS Security Hub Monitoring","text":""},{"location":"patterns/security/securityhub/#objective","title":"Objective","text":"<p>The objective of this pattern is to demonstrate how to enable Security Hub in your AWS account, verify that it is enabled, and get findings from Security Hub.</p> <p>The pattern will enable Security Hub in the <code>CDK_DEFAULT_ACCOUNT</code> and <code>CDK_DEFAULT_REGION</code>.</p>"},{"location":"patterns/security/securityhub/#prerequisites","title":"Prerequisites","text":"<ol> <li>Clone the repository</li> <li>Follow the usage instructions to install the dependencies</li> <li><code>argo-admin-password</code> secret must be defined in Secrets Manager in the same region as the EKS cluster.</li> <li>Complete the steps to enable AWS Config and deploy the Security Best Practices for Amazon EKS AWS Config managed rules.</li> </ol> <p>Optional (but recommended):  If you have not done so already, follow the steps to deploy the GuardDuty stack and blueprint. Since GuardDuty automatically sends its findings to Security Hub, the sample EKS finding will appear in Security Hub about five minutes after it has been enabled in the same region.</p>"},{"location":"patterns/security/securityhub/#deploy","title":"Deploy","text":"<p>To update npm, run the following command:</p> <pre><code>npm install -g npm@latest\n</code></pre> <p>To bootstrap the CDK toolkit and list all stacks in the app, run the following commands:</p> <pre><code>cdk bootstrap\nmake list\n</code></pre>"},{"location":"patterns/security/securityhub/#deploy-aws-security-hub","title":"Deploy AWS Security Hub","text":"<p>To enable Security Hub in the account and region deploy the stack, run the following command.</p> <pre><code>make pattern securityhub deploy securityhub-setup\n</code></pre> <p>Once deployed, AWS Security Hub will automatically enable the AWS Foundational Security Best Practices standard and the Center for Internet Security (CIS) AWS Foundations Benchmark v1.2.0 security standard controls status checks.</p>"},{"location":"patterns/security/securityhub/#verify","title":"Verify","text":""},{"location":"patterns/security/securityhub/#verify-that-security-hub-is-enabled","title":"Verify that Security Hub is enabled","text":"<p>Now you can check that Security Hub is successfully enabled by using the AWS CLI to query the same account and region.</p> <p>Using the AWS CLI run following command in the same account and region where you deployed the stack.</p> <pre><code>aws securityhub describe-hub\n</code></pre> <p>If you successfully enabled Security Hub, you will see the following.</p> <pre><code>{\n\"HubArn\": \"arn:aws:securityhub:us-east-1:XXXXXXXXXXXX:hub/default\",\n\"SubscribedAt\": \"2021-08-18T00:52:40.624Z\",\n\"AutoEnableControls\": true\n}\n</code></pre>"},{"location":"patterns/security/securityhub/#view-findings-in-security-hub","title":"View findings in Security Hub","text":"<p>The findings that you see in Security Hub will depend what you have configured in your account and region. In this example we deployed the GuardDuty EKS pattern, the Security Best Practices for Amazon EKS Config managed rules pattern, and successfully enabled Security Hub using the instructions above, which automatically enables two of the available Security Hub Security standard controls status checks.</p> <p>Use the following AWS CLI commands to view your findings in Security Hub.</p> <p>To list any critical findings, and findings related to controls that have a failed status according to Security Hub security standards which are enabled in the same account and region, run the following command.</p> <pre><code>aws securityhub get-findings --filter 'SeverityLabel={Value=CRITICAL,Comparison=EQUALS},ComplianceStatus={Value=FAILED,Comparison=EQUALS}'\n</code></pre> <p>The following is an example of an IAM finding that relates to a failed IAM control that Security Hub found related to the enabled Security standards, and will likely be present in your list of findings if you or your organization are not using a hardware MFA device for your AWS root account.</p> <pre><code>{\n\"Findings\": [\n{\n\"SchemaVersion\": \"2018-10-08\",\n\"Id\": \"arn:aws:securityhub:us-east-1:XXXXXXXXXXX:security-control/IAM.6/finding/494ffa38-0b6e-46d1-98f4-e605ec09d045\",\n\"ProductArn\": \"arn:aws:securityhub:us-east-1::product/aws/securityhub\",\n\"ProductName\": \"Security Hub\",\n\"CompanyName\": \"AWS\",\n\"Region\": \"us-east-1\",\n\"GeneratorId\": \"security-control/IAM.6\",\n\"AwsAccountId\": \"XXXXXXXXXXX\",\n\"Types\": [\n\"Software and Configuration Checks/Industry and Regulatory Standards\"\n],\n\"FirstObservedAt\": \"2023-03-04T00:54:44.307Z\",\n\"LastObservedAt\": \"2023-05-31T01:20:18.210Z\",\n\"CreatedAt\": \"2023-03-04T00:54:44.307Z\",\n\"UpdatedAt\": \"2023-05-31T01:20:05.845Z\",\n\"Severity\": {\n\"Label\": \"CRITICAL\",\n\"Normalized\": 90,\n\"Original\": \"CRITICAL\"\n},\n\"Title\": \"Hardware MFA should be enabled for the root user\",\n\"Description\": \"This AWS control checks whether your AWS account is enabled to use a hardware multi-factor authentication (MFA) device to sign in with root user credentials.\",\n\"Remediation\": {\n\"Recommendation\": {\n\"Text\": \"For information on how to correct this issue, consult the AWS Security Hub controls documentation.\",\n\"Url\": \"https://docs.aws.amazon.com/console/securityhub/IAM.6/remediation\"\n}\n},\n\"ProductFields\": {\n\"RelatedAWSResources:0/name\": \"securityhub-root-account-hardware-mfa-enabled-24e3b344\",\n\"RelatedAWSResources:0/type\": \"AWS::Config::ConfigRule\",\n\"aws/securityhub/ProductName\": \"Security Hub\",\n\"aws/securityhub/CompanyName\": \"AWS\",\n\"Resources:0/Id\": \"arn:aws:iam::XXXXXXXXXXX:root\",\n\"aws/securityhub/FindingId\": \"arn:aws:securityhub:us-east-1::product/aws/securityhub/arn:aws:securityhub:us-east-1:XXXXXXXXXXX:security-control/IAM.6/finding/494ffa38-0b6e-46d1-98f4-e605ec09d045\"\n},\n\"Resources\": [\n{\n\"Type\": \"AwsAccount\",\n\"Id\": \"AWS::::Account:XXXXXXXXXXX\",\n\"Partition\": \"aws\",\n\"Region\": \"us-east-1\"\n}\n],\n\"Compliance\": {\n\"Status\": \"FAILED\",\n\"RelatedRequirements\": [\n\"CIS AWS Foundations Benchmark v1.2.0/1.14\",\n\"CIS AWS Foundations Benchmark v1.4.0/1.6\",\n\"NIST.800-53.r5 AC-2(1)\",\n\"NIST.800-53.r5 AC-3(15)\",\n\"NIST.800-53.r5 IA-2(1)\",\n\"NIST.800-53.r5 IA-2(2)\",\n\"NIST.800-53.r5 IA-2(6)\",\n\"NIST.800-53.r5 IA-2(8)\",\n\"PCI DSS v3.2.1/8.3.1\"\n],\n\"SecurityControlId\": \"IAM.6\",\n\"AssociatedStandards\": [\n{\n\"StandardsId\": \"ruleset/cis-aws-foundations-benchmark/v/1.2.0\"\n},\n{\n\"StandardsId\": \"standards/aws-foundational-security-best-practices/v/1.0.0\"\n},\n{\n\"StandardsId\": \"standards/cis-aws-foundations-benchmark/v/1.4.0\"\n},\n{\n\"StandardsId\": \"standards/nist-800-53/v/5.0.0\"\n},\n{\n\"StandardsId\": \"standards/pci-dss/v/3.2.1\"\n}\n]\n},\n\"WorkflowState\": \"NEW\",\n\"Workflow\": {\n\"Status\": \"NEW\"\n},\n\"RecordState\": \"ACTIVE\",\n\"FindingProviderFields\": {\n\"Severity\": {\n\"Label\": \"CRITICAL\",\n\"Original\": \"CRITICAL\"\n},\n\"Types\": [\n\"Software and Configuration Checks/Industry and Regulatory Standards\"\n]\n}\n}\n]\n}\n</code></pre> <p>Now search for a finding related to the Security Best Practices for Amazon EKS Config managed rules, run the following AWS CLI command.</p> <pre><code>aws securityhub get-findings --filters 'GeneratorId={Value=\"security-control/EKS.1\", Comparison=\"EQUALS\"}'\n</code></pre> <p>You might see a finding such as the following.</p> <pre><code>{\n\"Findings\": [\n{\n\"SchemaVersion\": \"2018-10-08\",\n\"Id\": \"arn:aws:securityhub:us-east-1:XXXXXXXXXXX:security-control/EKS.1/finding/931a06d9-1b1d-431b-8b91-1ff86829b400\",\n\"ProductArn\": \"arn:aws:securityhub:us-east-1::product/aws/securityhub\",\n\"ProductName\": \"Security Hub\",\n\"CompanyName\": \"AWS\",\n\"Region\": \"us-east-1\",\n\"GeneratorId\": \"security-control/EKS.1\",\n\"AwsAccountId\": \"XXXXXXXXXXX\",\n\"Types\": [\n\"Software and Configuration Checks/Industry and Regulatory Standards\"\n],\n\"FirstObservedAt\": \"2023-05-09T10:34:36.736Z\",\n\"LastObservedAt\": \"2023-05-30T10:27:41.205Z\",\n\"CreatedAt\": \"2023-05-09T10:34:36.736Z\",\n\"UpdatedAt\": \"2023-05-30T10:27:34.574Z\",\n\"Severity\": {\n\"Label\": \"HIGH\",\n\"Normalized\": 70,\n\"Original\": \"HIGH\"\n},\n\"Title\": \"EKS cluster endpoints should not be publicly accessible\",\n\"Description\": \"This control checks whether an Amazon EKS cluster endpoint is publicly accessible. The control fails if an EKS cluster has an endpoint that is publicly accessible.\",\n\"Remediation\": {\n\"Recommendation\": {\n\"Text\": \"For information on how to correct this issue, consult the AWS Security Hub controls documentation.\",\n\"Url\": \"https://docs.aws.amazon.com/console/securityhub/EKS.1/remediation\"\n}\n},\n\"ProductFields\": {\n\"RelatedAWSResources:0/name\": \"securityhub-eks-endpoint-no-public-access-f5aecad6\",\n\"RelatedAWSResources:0/type\": \"AWS::Config::ConfigRule\",\n\"aws/securityhub/ProductName\": \"Security Hub\",\n\"aws/securityhub/CompanyName\": \"AWS\",\n\"aws/securityhub/annotation\": \"Cluster Endpoint of starter-blueprint is Publicly accessible\",\n\"Resources:0/Id\": \"arn:aws:eks:us-east-1:XXXXXXXXXXX:cluster/starter-blueprint\",\n\"aws/securityhub/FindingId\": \"arn:aws:securityhub:us-east-1::product/aws/securityhub/arn:aws:securityhub:us-east-1:XXXXXXXXXXX:security-control/EKS.1/finding/931a06d9-1b1d-431b-8b91-1ff86829b400\"\n},\n\"Resources\": [\n{\n\"Type\": \"AwsEksCluster\",\n\"Id\": \"arn:aws:eks:us-east-1:XXXXXXXXXXX:cluster/starter-blueprint\",\n\"Partition\": \"aws\",\n\"Region\": \"us-east-1\"\n}\n],\n\"Compliance\": {\n\"Status\": \"FAILED\",\n\"RelatedRequirements\": [\n\"NIST.800-53.r5 AC-21\",\n\"NIST.800-53.r5 AC-3\",\n\"NIST.800-53.r5 AC-3(7)\",\n\"NIST.800-53.r5 AC-4\",\n\"NIST.800-53.r5 AC-4(21)\",\n\"NIST.800-53.r5 AC-6\",\n\"NIST.800-53.r5 SC-7\",\n\"NIST.800-53.r5 SC-7(11)\",\n\"NIST.800-53.r5 SC-7(16)\",\n\"NIST.800-53.r5 SC-7(20)\",\n\"NIST.800-53.r5 SC-7(21)\",\n\"NIST.800-53.r5 SC-7(3)\",\n\"NIST.800-53.r5 SC-7(4)\",\n\"NIST.800-53.r5 SC-7(9)\"\n],\n\"SecurityControlId\": \"EKS.1\",\n\"AssociatedStandards\": [\n{\n\"StandardsId\": \"standards/aws-foundational-security-best-practices/v/1.0.0\"\n},\n{\n\"StandardsId\": \"standards/nist-800-53/v/5.0.0\"\n}\n]\n},\n\"WorkflowState\": \"NEW\",\n\"Workflow\": {\n\"Status\": \"NEW\"\n},\n\"RecordState\": \"ACTIVE\",\n\"FindingProviderFields\": {\n\"Severity\": {\n\"Label\": \"HIGH\",\n\"Original\": \"HIGH\"\n},\n\"Types\": [\n\"Software and Configuration Checks/Industry and Regulatory Standards\"\n]\n}\n}\n\n]\n}\n</code></pre> <p>To see any findings generated by GuardDuty in Security Hub, run the following command.</p> <pre><code>aws securityhub get-findings --filters 'ProductName={Value=\"GuardDuty\",Comparison=\"EQUALS\"}'\n</code></pre> <pre><code>{\n\"Findings\": [\n{\n\"SchemaVersion\": \"2018-10-08\",\n\"Id\": \"arn:aws:guardduty:us-east-1:XXXXXXXXXXX:detector/68b6db88cfef1e59333ecbccd8e816b5/finding/0ec437473c147f649d1437f94d615224\",\n\"ProductArn\": \"arn:aws:securityhub:us-east-1::product/aws/guardduty\",\n\"ProductName\": \"GuardDuty\",\n\"CompanyName\": \"Amazon\",\n\"Region\": \"us-east-1\",\n\"GeneratorId\": \"arn:aws:guardduty:us-east-1:XXXXXXXXXXX:detector/68b6db88cfef1e59333ecbccd8e816b5\",\n\"AwsAccountId\": \"XXXXXXXXXXX\",\n\"Types\": [\n\"TTPs/PrivilegeEscalation/PrivilegeEscalation:Kubernetes-PrivilegedContainer\"\n],\n...\n\"Severity\": {\n\"Product\": 5,\n\"Label\": \"MEDIUM\",\n\"Normalized\": 50\n},\n\"Title\": \"Privileged container with root level access launched on the EKS Cluster.\",\n\"Description\": \"A privileged container with root level access was launched on EKS Cluster guardduty-blueprint. If this behavior is not expected, it may indicate that your credentials are compromised.\",\n\"SourceUrl\": \"https://us-east-1.console.aws.amazon.com/guardduty/home?region=us-east-1#/findings?macros=current&amp;fId=0ec437473c147f649d1437f94d615224\",\n\"ProductFields\": {\n...\n},\n\"Resources\": [\n{ ... }\n],\n\"WorkflowState\": \"NEW\",\n\"Workflow\": {\n\"Status\": \"NEW\"\n},\n\"RecordState\": \"ACTIVE\",\n\"FindingProviderFields\": {\n\"Severity\": {\n\"Label\": \"MEDIUM\"\n},\n\"Types\": [\n\"TTPs/PrivilegeEscalation/PrivilegeEscalation:Kubernetes-PrivilegedContainer\"\n]\n},\n\"Sample\": false\n}\n]\n}\n</code></pre> <p>If you deployed the Amazon GuardDuty Protection EKS Blueprints pattern to the same account and region where you enabled Security Hub you should see a GuardDuty finding like the one above. The sample workload deployed with the GuardDuty pattern which contains a privileged container is detected by GuardDuty and generates the <code>Kubernetes-PrivilegedContainer</code> finding. GuardDuty automatically sent this finding to Security Hub where it can be viewed and triaged.</p>"}]}